{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9826841",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, math, json, copy, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "PG_HOST    = \"119.148.17.102\"\n",
    "PG_PORT    = 5432\n",
    "PG_DB      = \"ewarsdb\"\n",
    "PG_USER    = \"ewars\"\n",
    "PG_PASS    = \"Iedcr@Ewars2025\"\n",
    "PG_SSLMODE = \"require\"\n",
    "\n",
    "OUT_ROOT = \"/content/malaria_out\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "SEED=42\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model / training\n",
    "SEQ=12                  # lookback window length (months)\n",
    "BATCH=64\n",
    "EPOCHS=350\n",
    "PATIENCE=25\n",
    "LSTM_UNITS=128\n",
    "HEADS=8\n",
    "DROP=0.25\n",
    "LR=8e-4\n",
    "WD=1e-4\n",
    "CLIP=1.0\n",
    "TF_START=1.0\n",
    "TF_END=0.3\n",
    "Q=(0.1,0.5,0.9)\n",
    "\n",
    "# split lengths per upazila\n",
    "VAL_H_MONTHS  = 6\n",
    "TEST_H_MONTHS = 6\n",
    "\n",
    "# classification thresholds / dashboard logic (reuse diarrhoea style)\n",
    "DEFAULT_CAPACITY_PER_DISTRICT = 25\n",
    "RAPID_GROWTH_THRESH = 0.30     # >=30% wow growth => red\n",
    "MODERATE_GROWTH_THRESH = 0.10  # >=10% wow growth => orange\n",
    "\n",
    "PEAK_LOOKAHEAD_DAYS = 15       # same placeholder logic as diarrhoea (we'll treat it as ~2 weeks window)\n",
    "CONF_HIGH = 5\n",
    "CONF_MED  = 15\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "\n",
    "# which climate/env features we keep\n",
    "BASE_FEATURES = [\n",
    "    \"average_temperature\",\n",
    "    \"total_rainfall\",\n",
    "    \"relative_humidity\",\n",
    "    \"average_ndvi\",\n",
    "    \"average_ndwi\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# DB HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def get_db_conn():\n",
    "    dsn = (\n",
    "        f\"host={PG_HOST} port={PG_PORT} dbname={PG_DB} \"\n",
    "        f\"user={PG_USER} password={PG_PASS} sslmode={PG_SSLMODE}\"\n",
    "    )\n",
    "    return psycopg2.connect(dsn)\n",
    "\n",
    "def _dedupe_by_pk(rows, cols, pk_cols):\n",
    "    if not pk_cols:\n",
    "        return rows\n",
    "    pk_idx = [cols.index(pk) for pk in pk_cols]\n",
    "    dedup = {}\n",
    "    for row in rows:\n",
    "        key = tuple(row[i] for i in pk_idx)\n",
    "        dedup[key] = row\n",
    "    return list(dedup.values())\n",
    "\n",
    "def upsert_table(conn, table_name, cols, rows, pk_cols=None, wipe_first=False):\n",
    "    if wipe_first:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"TRUNCATE TABLE {table_name};\")\n",
    "        conn.commit()\n",
    "    if not rows:\n",
    "        return\n",
    "    if not pk_cols:\n",
    "        with conn.cursor() as cur:\n",
    "            insert_sql = f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s\"\n",
    "            execute_values(cur, insert_sql, rows)\n",
    "        conn.commit()\n",
    "        return\n",
    "\n",
    "    rows_dedup = _dedupe_by_pk(rows, cols, pk_cols)\n",
    "    conflict_target = \", \".join(pk_cols)\n",
    "    set_updates = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in cols if c not in pk_cols])\n",
    "    insert_sql = (\n",
    "        f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s \"\n",
    "        f\"ON CONFLICT ({conflict_target}) DO UPDATE SET {set_updates};\"\n",
    "    )\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, insert_sql, rows_dedup)\n",
    "    conn.commit()\n",
    "\n",
    "def to_python(v):\n",
    "    if isinstance(v, (np.floating, np.float32, np.float64)):\n",
    "        v = float(v)\n",
    "        if math.isnan(v):\n",
    "            return None\n",
    "        return v\n",
    "    if isinstance(v, (np.integer, np.int32, np.int64)):\n",
    "        return int(v)\n",
    "    if isinstance(v, (np.bool_,)):\n",
    "        return bool(v)\n",
    "    if isinstance(v, float) and math.isnan(v):\n",
    "        return None\n",
    "    if isinstance(v, dict):\n",
    "        return {k: to_python(x) for k,x in v.items()}\n",
    "    if isinstance(v, (list, tuple)):\n",
    "        return [to_python(x) for x in v]\n",
    "    return v\n",
    "\n",
    "def py(v):\n",
    "    if isinstance(v, (np.floating, np.float32, np.float64)):\n",
    "        v = float(v)\n",
    "    elif isinstance(v, (np.integer, np.int32, np.int64)):\n",
    "        v = int(v)\n",
    "    elif isinstance(v, (np.bool_,)):\n",
    "        v = bool(v)\n",
    "    if isinstance(v, float) and math.isnan(v):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "# ============================================================\n",
    "# CREATE OUTPUT TABLES (PER TARGET PREFIX)\n",
    "# ============================================================\n",
    "\n",
    "def ensure_output_tables_malaria(conn, prefix):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # drop in reverse dependency-ish order just to be safe\n",
    "    table_suffixes = [\n",
    "        \"exec_summary\",\n",
    "        \"climate_influence\",\n",
    "        \"nowcast_gap\",\n",
    "        \"isochrone_spread\",\n",
    "        \"peak_projection\",\n",
    "        \"surveillance_quality\",\n",
    "        \"confidence\",\n",
    "        \"acceleration_alerts\",\n",
    "        \"overflow_risk\",\n",
    "        \"watchlist\",\n",
    "    ]\n",
    "\n",
    "    for suf in table_suffixes:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {prefix}_{suf} CASCADE;\")\n",
    "\n",
    "    # WATCHLIST\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_watchlist (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        expected_cases_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        status TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # OVERFLOW RISK\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_overflow_risk (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        capacity_threshold_beds_per_week INT,\n",
    "        forecast_median_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        breach_risk_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # ACCELERATION ALERTS\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_acceleration_alerts (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        last_week_cases NUMERIC,\n",
    "        this_week_actual NUMERIC,\n",
    "        this_week_predicted NUMERIC,\n",
    "        next_week_forecast NUMERIC,\n",
    "        growth_rate_wow NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # CONFIDENCE\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_confidence (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        forecast_next_week NUMERIC,\n",
    "        uncertainty_width NUMERIC,\n",
    "        confidence_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # SURVEILLANCE QUALITY\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_surveillance_quality (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        reporting_continuity_pct NUMERIC,\n",
    "        data_quality_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # PEAK PROJECTION\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_peak_projection (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        peak_lead_time_weeks NUMERIC,\n",
    "        peak_cases_median NUMERIC,\n",
    "        peak_cases_high_p90 NUMERIC,\n",
    "        peak_when TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # ISOCHRONE SPREAD\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_isochrone_spread (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        forecast_next_week_median NUMERIC,\n",
    "        forecast_next_week_hi NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # NOWCAST GAP\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_nowcast_gap (\n",
    "        district TEXT NOT NULL,\n",
    "        upazila  TEXT NOT NULL,\n",
    "        year INT NOT NULL,\n",
    "        month INT NOT NULL,\n",
    "        current_week_actual NUMERIC,\n",
    "        current_week_predicted_from_prev NUMERIC,\n",
    "        nowcast_gap_percent NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, upazila, year, month)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # CLIMATE INFLUENCE\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_climate_influence (\n",
    "        feature TEXT,\n",
    "        base_var TEXT,\n",
    "        lag_info TEXT,\n",
    "        pearson_corr_with_next_week_forecast NUMERIC,\n",
    "        abs_corr NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # EXEC SUMMARY\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE {prefix}_exec_summary (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        summary JSONB,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "  \n",
    "# ============================================================\n",
    "# DATA LOAD + FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "def load_malaria_monthly_from_db():\n",
    "    \"\"\"\n",
    "    Pull raw monthly malaria + climate from Postgres malaria_weather.\n",
    "    We'll clean, compute per-1k rates, fill gaps.\n",
    "    Required columns:\n",
    "    dis_name, upa_name, upazilaid, year, month,\n",
    "    pv, pf, population,\n",
    "    average_temperature, total_rainfall, relative_humidity,\n",
    "    average_ndvi, average_ndwi.\n",
    "    \"\"\"\n",
    "    conn = get_db_conn()\n",
    "    try:\n",
    "        sql = \"\"\"\n",
    "            SELECT\n",
    "                dis_name,\n",
    "                upa_name,\n",
    "                upazilaid,\n",
    "                year,\n",
    "                month,\n",
    "                pv,\n",
    "                pf,\n",
    "                population,\n",
    "                average_temperature,\n",
    "                total_rainfall,\n",
    "                relative_humidity,\n",
    "                average_ndvi,\n",
    "                average_ndwi\n",
    "            FROM malaria_weather\n",
    "            WHERE year IS NOT NULL\n",
    "              AND month IS NOT NULL\n",
    "            ORDER BY upazilaid, year, month;\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(sql, conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # normalize\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    df[\"upazilaid\"] = pd.to_numeric(df[\"upazilaid\"], errors=\"coerce\")\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    for c in [\"pv\",\"pf\",\"population\",\n",
    "              \"average_temperature\",\"total_rainfall\",\"relative_humidity\",\n",
    "              \"average_ndvi\",\"average_ndwi\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # rate per 1000 pop\n",
    "    df[\"pv_rate_1k\"] = np.where(\n",
    "        (df[\"population\"]>0) & df[\"pv\"].notna(),\n",
    "        1000.0 * df[\"pv\"] / df[\"population\"],\n",
    "        np.nan\n",
    "    )\n",
    "    df[\"pf_rate_1k\"] = np.where(\n",
    "        (df[\"population\"]>0) & df[\"pf\"].notna(),\n",
    "        1000.0 * df[\"pf\"] / df[\"population\"],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # chronological key\n",
    "    df[\"ym\"] = df[\"year\"].astype(int)*12 + df[\"month\"].astype(int)\n",
    "\n",
    "    # forward fill climate + population + targets per upazila\n",
    "    fcols = BASE_FEATURES + [\"population\",\"pv_rate_1k\",\"pf_rate_1k\"]\n",
    "    for col in fcols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    df = df.sort_values([\"upazilaid\",\"year\",\"month\"])\n",
    "    df[fcols] = (\n",
    "        df.groupby(\"upazilaid\")[fcols]\n",
    "          .apply(lambda g: g.ffill())\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # replace inf -> NaN -> ffill again\n",
    "    df.replace([np.inf,-np.inf], np.nan, inplace=True)\n",
    "    df[fcols] = (\n",
    "        df.groupby(\"upazilaid\")[fcols]\n",
    "          .apply(lambda g: g.ffill())\n",
    "          .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # seasonality embeddings\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"].astype(float)/12.0).astype(np.float32)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"].astype(float)/12.0).astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_lags_rolls_monthly(df, group_col=\"upazilaid\",\n",
    "                           base_cols=BASE_FEATURES,\n",
    "                           lags=(1,3,6),\n",
    "                           rolls=(3,6)):\n",
    "    \"\"\"\n",
    "    Adds lag and rolling stats for each base col.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([group_col,\"year\",\"month\"]).copy()\n",
    "    new_cols = []\n",
    "    for c in base_cols:\n",
    "        for L in lags:\n",
    "            col = f\"{c}_lag{L}\"\n",
    "            df[col] = df.groupby(group_col)[c].shift(L)\n",
    "            new_cols.append(col)\n",
    "        for R in rolls:\n",
    "            m = f\"{c}_rmean{R}\"\n",
    "            s = f\"{c}_rstd{R}\"\n",
    "            g = df.groupby(group_col)[c]\n",
    "            df[m] = g.rolling(R, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            df[s] = g.rolling(R, min_periods=1).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            new_cols += [m,s]\n",
    "    df[new_cols] = df[new_cols].fillna(0.0)\n",
    "    return df, new_cols\n",
    "\n",
    "# ============================================================\n",
    "# SEQUENCE BUILDING / SCALING\n",
    "# ============================================================\n",
    "\n",
    "class Split:\n",
    "    def __init__(self, X, y, c, df):\n",
    "        self.X=X; self.y=y; self.c=c; self.df=df\n",
    "\n",
    "def build_mats(df, feat_cols, target_col):\n",
    "    \"\"\"\n",
    "    df must have:\n",
    "      - upazilaid\n",
    "      - year, month\n",
    "      - target_col (pv_rate_1k or pf_rate_1k)\n",
    "    We encode upazilaid -> entity_id.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder().fit(df[\"upazilaid\"].astype(str).values)\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"entity_id\"] = le.transform(d[\"upazilaid\"].astype(str))\n",
    "\n",
    "    X = d[feat_cols].values.astype(np.float32)\n",
    "    feat_scaler = StandardScaler().fit(X)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    # log1p target (non-negative)\n",
    "    y_raw = np.log1p(np.clip(d[target_col].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y_raw, np.float32)\n",
    "    y_scalers = {}\n",
    "    for cid, g in d.groupby(\"entity_id\"):\n",
    "        idx = g.index.values\n",
    "        sc  = StandardScaler().fit(y_raw[idx])\n",
    "        y_scalers[cid] = sc\n",
    "        Ys[idx] = sc.transform(y_raw[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"entity_id\"].values.astype(np.int64), d), le, feat_scaler, y_scalers\n",
    "\n",
    "def apply_mats(df, feat_cols, target_col, le, feat_scaler, y_scalers):\n",
    "    d = df.copy()\n",
    "    d[\"entity_id\"] = le.transform(d[\"upazilaid\"].astype(str))\n",
    "    X = d[feat_cols].values.astype(np.float32)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y_raw = np.log1p(np.clip(d[target_col].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y_raw, np.float32)\n",
    "    for cid, g in d.groupby(\"entity_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = y_scalers.get(int(cid), StandardScaler().fit(y_raw[idx]))\n",
    "        Ys[idx] = sc.transform(y_raw[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"entity_id\"].values.astype(np.int64), d)\n",
    "\n",
    "def to_seq(split, L=SEQ, prev_y=True):\n",
    "    \"\"\"\n",
    "    Build rolling SEQ-month windows for each upazila,\n",
    "    require strictly consecutive ym (year*12+month).\n",
    "    \"\"\"\n",
    "    X,y,c,df = split.X, split.y, split.c, split.df\n",
    "    ym = (df[\"year\"].astype(int)*12 + df[\"month\"].astype(int)).values\n",
    "\n",
    "    SX,SY,SC = [],[],[]\n",
    "    for cid in np.unique(c):\n",
    "        idx = np.where(c==cid)[0]\n",
    "        # sort within this upazila by ym\n",
    "        order = np.argsort(ym[idx])\n",
    "        idx   = idx[order]\n",
    "        o     = ym[idx]\n",
    "        for i in range(len(idx)-L+1):\n",
    "            sl = idx[i:i+L]\n",
    "            if np.all(np.diff(o[i:i+L]) == 1):\n",
    "                Xi = X[sl]\n",
    "                Yi = y[sl]\n",
    "                if prev_y:\n",
    "                    prev_col = np.vstack([np.zeros((1,1),np.float32), Yi[:-1]])\n",
    "                    Xi = np.concatenate([Xi, prev_col], axis=1)\n",
    "                SX.append(Xi)\n",
    "                SY.append(Yi)\n",
    "                SC.append(cid)\n",
    "    return (\n",
    "        np.asarray(SX, np.float32),\n",
    "        np.asarray(SY, np.float32),\n",
    "        np.asarray(SC, np.int64),\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# MODEL (LSTM + ATTENTION)\n",
    "# ============================================================\n",
    "\n",
    "class Forecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM + causal self-attention with upazila embedding;\n",
    "    outputs mean, log-sigma, and quantiles.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, n_entities,\n",
    "                 emb_ent=8, lstm=LSTM_UNITS, heads=HEADS, drop=DROP, qu=Q):\n",
    "        super().__init__()\n",
    "        self.q = qu\n",
    "        self.ent_emb = nn.Embedding(n_entities, emb_ent)\n",
    "\n",
    "        self.lstm = nn.LSTM(cond_dim + emb_ent, lstm, 1, batch_first=True)\n",
    "        self.mha  = nn.MultiheadAttention(lstm, heads, batch_first=True, dropout=drop)\n",
    "        self.ln   = nn.LayerNorm(lstm)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.mu = nn.Linear(lstm,1)\n",
    "        self.ls = nn.Linear(lstm,1)\n",
    "        self.qh = nn.ModuleList([nn.Linear(lstm,1) for _ in qu])\n",
    "\n",
    "        self.last_attn = None\n",
    "\n",
    "    def forward(self, cond, ent_id):\n",
    "        B,L,D = cond.shape\n",
    "        e = self.ent_emb(ent_id).unsqueeze(1).repeat(1,L,1)  # [B,L,emb_ent]\n",
    "        x = torch.cat([cond, e], dim=-1)\n",
    "\n",
    "        h,_ = self.lstm(x)\n",
    "\n",
    "        # causal mask so each month can't see future months\n",
    "        mask = torch.triu(torch.ones(L, L, device=h.device, dtype=torch.bool), diagonal=1)\n",
    "        att, w = self.mha(h,h,h, attn_mask=mask, need_weights=True)\n",
    "        self.last_attn = w.detach()\n",
    "\n",
    "        h = self.drop(self.ln(att))\n",
    "\n",
    "        mu = self.mu(h)\n",
    "        ls = torch.clamp(self.ls(h), -5.0, 3.0)\n",
    "        qs = [head(h) for head in self.qh]\n",
    "\n",
    "        return mu, ls, qs\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / VAL\n",
    "# ============================================================\n",
    "\n",
    "def smape(y, p):\n",
    "    y = y.flatten()\n",
    "    p = p.flatten()\n",
    "    return 100*np.mean(2*np.abs(p-y)/(np.abs(y)+np.abs(p)+1e-8))\n",
    "\n",
    "def pinball(pred, target, quantile):\n",
    "    err = target - pred\n",
    "    return torch.mean(torch.maximum(quantile * err, (quantile - 1) * err))\n",
    "\n",
    "def tv_l1_on_mu(mu):\n",
    "    diff = mu[:,1:,:] - mu[:,:-1,:]\n",
    "    return diff.abs().mean(), mu.abs().mean()\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(self,X,Y,C):\n",
    "        self.X=X; self.Y=Y; self.C=C\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,i):\n",
    "        return self.X[i], self.Y[i], self.C[i]\n",
    "\n",
    "def validate_model(M,Xv,Yv,Cv):\n",
    "    M.eval()\n",
    "    with torch.no_grad():\n",
    "        X  = torch.tensor(Xv, dtype=torch.float32, device=DEVICE)\n",
    "        C  = torch.tensor(Cv, dtype=torch.long,   device=DEVICE)\n",
    "        mu, ls, qs = M(X, C)\n",
    "\n",
    "        mp  = mu[...,0].cpu().numpy()\n",
    "        y   = Yv[...,0]\n",
    "        q10 = qs[0][...,0].cpu().numpy()\n",
    "        q90 = qs[2][...,0].cpu().numpy()\n",
    "\n",
    "        coverage = float(np.mean((y>=q10) & (y<=q90)))\n",
    "        score    = smape(y.reshape(-1), mp.reshape(-1))\n",
    "\n",
    "    return score, coverage\n",
    "\n",
    "def train_model(Xtr,Ytr,Ctr,\n",
    "                Xva,Yva,Cva,\n",
    "                cond_dim,nc):\n",
    "    M = Forecaster(cond_dim=cond_dim, n_entities=nc).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(M.parameters(),\n",
    "                            lr=LR,\n",
    "                            betas=(0.9,0.999),\n",
    "                            weight_decay=WD)\n",
    "\n",
    "    dl = DataLoader(SeqDS(Xtr,Ytr,Ctr),\n",
    "                    batch_size=BATCH,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "\n",
    "    best=float(\"inf\")\n",
    "    best_sd=None\n",
    "    wait=0\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        # scheduled sampling factor for prev_y channel\n",
    "        t  = e / max(1,(EPOCHS-1))\n",
    "        tf = TF_START + (TF_END-TF_START)*t\n",
    "\n",
    "        M.train()\n",
    "        for Xb,Yb,Cb in dl:\n",
    "            Xb  = Xb.to(torch.float32).to(DEVICE)\n",
    "            Yb  = Yb.to(torch.float32).to(DEVICE)\n",
    "            Cb  = Cb.to(torch.long).to(DEVICE)\n",
    "\n",
    "            B,L,_ = Xb.shape\n",
    "\n",
    "            # scheduled teacher forcing for prev_y (last feature in X)\n",
    "            with torch.no_grad():\n",
    "                mu0,_,_ = M(Xb,Cb)\n",
    "                prev = torch.cat(\n",
    "                    [torch.zeros(B,1,device=DEVICE), mu0[:,:-1,0]],\n",
    "                    dim=1\n",
    "                )\n",
    "            Xb[:,:,-1] = tf*Xb[:,:,-1] + (1-tf)*prev\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            mu, ls, qs = M(Xb,Cb)\n",
    "\n",
    "            sig = (ls.exp()).clamp(1e-3,50.0)\n",
    "            nll = 0.5*(((Yb-mu)/sig)**2 + 2*ls + math.log(2*math.pi)).mean()\n",
    "\n",
    "            ql = 0.0\n",
    "            for i, qv in enumerate(Q):\n",
    "                ql += pinball(qs[i],Yb,qv)\n",
    "            ql /= len(Q)\n",
    "\n",
    "            tv, l1 = tv_l1_on_mu(mu)\n",
    "            loss = 1.0*nll + 1.0*ql + 0.05*tv + 0.02*l1\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(M.parameters(),CLIP)\n",
    "            opt.step()\n",
    "\n",
    "        sm, cov = validate_model(M,Xva,Yva,Cva)\n",
    "        comp = sm + 10*abs(cov-0.9)\n",
    "\n",
    "        if comp < best-1e-6:\n",
    "            best    = comp\n",
    "            best_sd = copy.deepcopy(M.state_dict())\n",
    "            wait    = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_sd is not None:\n",
    "        M.load_state_dict(best_sd)\n",
    "    return M\n",
    "\n",
    "# ============================================================\n",
    "# DASHBOARD HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def inverse_seq(arr_scaled, scaler):\n",
    "    arr_scaled = np.array(arr_scaled).reshape(-1,1)\n",
    "    unstd = scaler.inverse_transform(arr_scaled).reshape(-1)\n",
    "    return np.clip(np.expm1(unstd), 0, None)\n",
    "\n",
    "def classify_growth(g):\n",
    "    if g >= RAPID_GROWTH_THRESH:\n",
    "        return \"🔴 Rapid growth\"\n",
    "    elif g >= MODERATE_GROWTH_THRESH:\n",
    "        return \"🟠 Moderate growth\"\n",
    "    else:\n",
    "        return \"🟢 Stable/decline\"\n",
    "\n",
    "def classify_risk(high_scenario):\n",
    "    if high_scenario >= 40:\n",
    "        return \"🔴 Surge likely\"\n",
    "    elif high_scenario >= 20:\n",
    "        return \"🟠 Elevated\"\n",
    "    else:\n",
    "        return \"🟢 Low\"\n",
    "\n",
    "def classify_conf(width):\n",
    "    if width <= CONF_HIGH:\n",
    "        return \"✅ High confidence\"\n",
    "    elif width <= CONF_MED:\n",
    "        return \"🟡 Medium confidence\"\n",
    "    else:\n",
    "        return \"⚠ Low confidence\"\n",
    "\n",
    "def classify_data_quality(continuity_pct):\n",
    "    if continuity_pct >= 90:\n",
    "        return \"✅ Reliable\"\n",
    "    elif continuity_pct >= 70:\n",
    "        return \"🟡 Watch\"\n",
    "    else:\n",
    "        return \"🔴 Needs field check\"\n",
    "\n",
    "def evaluate_and_build_products_malaria(\n",
    "    M,\n",
    "    Xseq, Yseq, Cseq,\n",
    "    y_scalers, le,\n",
    "    seq_lookup_df,\n",
    "    feat_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    Build director-style products for malaria.\n",
    "    - each seq = 1 upazila\n",
    "    - attach dis_name, upa_name, year, month\n",
    "    - derive \"weekly\" numbers by *7 scaling (placeholder)\n",
    "    \"\"\"\n",
    "\n",
    "    M.eval()\n",
    "    X_t = torch.tensor(Xseq, dtype=torch.float32, device=DEVICE)\n",
    "    C_t = torch.tensor(Cseq, dtype=torch.long,   device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, ls, qs = M(X_t, C_t)\n",
    "        mu_np  = mu.cpu().numpy()[...,0]      # [N,L]\n",
    "        q10_np = qs[0].cpu().numpy()[...,0]\n",
    "        q50_np = qs[1].cpu().numpy()[...,0]\n",
    "        q90_np = qs[2].cpu().numpy()[...,0]\n",
    "\n",
    "    idx_curr = -2\n",
    "    idx_next = -1\n",
    "\n",
    "    district_names = []\n",
    "    upazila_names  = []\n",
    "    year_list      = []\n",
    "    month_list     = []\n",
    "\n",
    "    last_week_cases_list = []\n",
    "    this_week_actual_list = []\n",
    "    this_week_pred_list   = []\n",
    "    next_week_forecast_list = []\n",
    "    next_week_hi_list       = []\n",
    "    next_week_lo_list       = []\n",
    "    growth_rate_list        = []\n",
    "    conf_width_list         = []\n",
    "\n",
    "    peak_day_list = []\n",
    "    peak_val_list = []\n",
    "    peak_hi_list  = []\n",
    "    peak_leadtime_list = []\n",
    "\n",
    "    all_y_real = []\n",
    "    all_mu_real = []\n",
    "    all_lo_real = []\n",
    "    all_hi_real = []\n",
    "\n",
    "    last_meta = {}  # key=(district,upazila)\n",
    "\n",
    "    for i, cid in enumerate(Cseq):\n",
    "        sc = y_scalers[int(cid)]\n",
    "        upa_df = seq_lookup_df[int(cid)]\n",
    "        upa_df = upa_df.sort_values([\"year\",\"month\"])\n",
    "\n",
    "        y_real_full   = inverse_seq(Yseq[i].reshape(-1),  sc)\n",
    "        mu_real_full  = inverse_seq(mu_np[i].reshape(-1), sc)\n",
    "        lo_real_full  = inverse_seq(q10_np[i].reshape(-1),sc)\n",
    "        hi_real_full  = inverse_seq(q90_np[i].reshape(-1),sc)\n",
    "\n",
    "        all_y_real.append(y_real_full)\n",
    "        all_mu_real.append(mu_real_full)\n",
    "        all_lo_real.append(lo_real_full)\n",
    "        all_hi_real.append(hi_real_full)\n",
    "\n",
    "        safe_idx_curr = idx_curr if idx_curr >= -len(y_real_full) else -1\n",
    "        safe_idx_next = idx_next if idx_next >= -len(y_real_full) else -1\n",
    "\n",
    "        actual_curr  = y_real_full[safe_idx_curr]\n",
    "        pred_curr    = mu_real_full[safe_idx_curr]\n",
    "        if (safe_idx_curr-1)>=-len(y_real_full):\n",
    "            actual_prev = y_real_full[safe_idx_curr-1]\n",
    "        else:\n",
    "            actual_prev = y_real_full[safe_idx_curr]\n",
    "\n",
    "        pred_next    = mu_real_full[safe_idx_next]\n",
    "        hi_next      = hi_real_full[safe_idx_next]\n",
    "        lo_next      = lo_real_full[safe_idx_next]\n",
    "\n",
    "        growth      = (pred_next - actual_prev) / (actual_prev + 1e-6)\n",
    "        conf_width  = hi_next - lo_next\n",
    "\n",
    "        # crude peak projection on last PEAK_LOOKAHEAD_DAYS steps\n",
    "        look_slice = mu_real_full[-PEAK_LOOKAHEAD_DAYS:]\n",
    "        hi_slice   = hi_real_full[-PEAK_LOOKAHEAD_DAYS:]\n",
    "        if len(look_slice)==0:\n",
    "            peak_when=\"NA\"; peak_val=np.nan; peak_hi=np.nan; lead_weeks=np.nan\n",
    "        else:\n",
    "            local_max_idx = int(np.argmax(look_slice))\n",
    "            peak_val = look_slice[local_max_idx]\n",
    "            peak_hi  = hi_slice[local_max_idx]\n",
    "            lead_weeks = ((len(look_slice)-1) - local_max_idx)/7.0\n",
    "            peak_when = f\"t+{(len(look_slice)-1 - local_max_idx)}d\"\n",
    "\n",
    "        # latest metadata for this upazila\n",
    "        lastrow = upa_df.iloc[-1]\n",
    "        dis_name  = str(lastrow[\"dis_name\"])\n",
    "        upa_name  = str(lastrow[\"upa_name\"])\n",
    "        yval      = int(lastrow[\"year\"])\n",
    "        mval      = int(lastrow[\"month\"])\n",
    "\n",
    "        # scale to \"weekly-style\" numbers for dashboards (placeholder)\n",
    "        next_week_forecast = pred_next * 7.0\n",
    "        next_week_hi       = hi_next   * 7.0\n",
    "        next_week_lo       = lo_next   * 7.0\n",
    "        this_week_pred     = pred_curr * 7.0\n",
    "        last_week_cases    = actual_prev * 7.0\n",
    "        this_week_actual   = actual_curr * 7.0\n",
    "\n",
    "        district_names.append(dis_name)\n",
    "        upazila_names.append(upa_name)\n",
    "        year_list.append(yval)\n",
    "        month_list.append(mval)\n",
    "\n",
    "        last_week_cases_list.append(last_week_cases)\n",
    "        this_week_actual_list.append(this_week_actual)\n",
    "        this_week_pred_list.append(this_week_pred)\n",
    "        next_week_forecast_list.append(next_week_forecast)\n",
    "        next_week_hi_list.append(next_week_hi)\n",
    "        next_week_lo_list.append(next_week_lo)\n",
    "        growth_rate_list.append(growth)\n",
    "        conf_width_list.append(next_week_hi - next_week_lo)\n",
    "\n",
    "        peak_day_list.append(peak_when)\n",
    "        peak_val_list.append(peak_val * 7.0)\n",
    "        peak_hi_list.append(peak_hi * 7.0)\n",
    "        peak_leadtime_list.append(lead_weeks)\n",
    "\n",
    "        last_meta[(dis_name, upa_name)] = {\n",
    "            \"Year\": yval,\n",
    "            \"Month\": mval\n",
    "        }\n",
    "\n",
    "    all_y_real  = np.concatenate(all_y_real)\n",
    "    all_mu_real = np.concatenate(all_mu_real)\n",
    "    all_lo_real = np.concatenate(all_lo_real)\n",
    "    all_hi_real = np.concatenate(all_hi_real)\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"SMAPE\": smape(all_y_real, all_mu_real),\n",
    "        \"MSE\": mean_squared_error(all_y_real, all_mu_real),\n",
    "        \"RMSE\": math.sqrt(mean_squared_error(all_y_real, all_mu_real)),\n",
    "        \"R2\": r2_score(all_y_real, all_mu_real),\n",
    "        \"Coverage90\": float(np.mean((all_y_real >= all_lo_real) & (all_y_real <= all_hi_real))),\n",
    "    }\n",
    "\n",
    "    # build tables\n",
    "\n",
    "    watchlist_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Expected_cases_next_week\": np.round(next_week_forecast_list,2),\n",
    "        \"High_scenario_p90\":        np.round(next_week_hi_list,2),\n",
    "    })\n",
    "    watchlist_df[\"Status\"] = watchlist_df[\"High_scenario_p90\"].apply(classify_risk)\n",
    "\n",
    "    capacity_lookup = {\n",
    "        (d,u): DEFAULT_CAPACITY_PER_DISTRICT\n",
    "        for d,u in zip(district_names, upazila_names)\n",
    "    }\n",
    "    overflow_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Capacity_threshold_beds_per_week\": [\n",
    "            capacity_lookup[(d,u)]\n",
    "            for d,u in zip(district_names, upazila_names)\n",
    "        ],\n",
    "        \"Forecast_median_next_week\": np.round(next_week_forecast_list,2),\n",
    "        \"High_scenario_p90\":         np.round(next_week_hi_list,2),\n",
    "    })\n",
    "    overflow_df[\"Breach_risk_flag\"] = [\n",
    "        \"YES\" if hi > capacity_lookup[(d,u)] else \"NO\"\n",
    "        for (d,u), hi in zip(zip(district_names,upazila_names), next_week_hi_list)\n",
    "    ]\n",
    "\n",
    "    accel_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Last_week_cases\":        np.round(last_week_cases_list,2),\n",
    "        \"This_week_actual\":       np.round(this_week_actual_list,2),\n",
    "        \"This_week_predicted\":    np.round(this_week_pred_list,2),\n",
    "        \"Next_week_forecast\":     np.round(next_week_forecast_list,2),\n",
    "        \"Growth_rate_WoW(%)\":     np.round(np.array(growth_rate_list)*100,1),\n",
    "    })\n",
    "    accel_df[\"Growth_flag\"] = accel_df[\"Growth_rate_WoW(%)\"].apply(\n",
    "        lambda pct: classify_growth(pct/100.0)\n",
    "    )\n",
    "\n",
    "    confidence_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Forecast_next_week\":            np.round(next_week_forecast_list,2),\n",
    "        \"Uncertainty_width(p90-p10)\":    np.round(conf_width_list,2),\n",
    "    })\n",
    "    confidence_df[\"Confidence_flag\"] = confidence_df[\"Uncertainty_width(p90-p10)\"].apply(classify_conf)\n",
    "\n",
    "    # simple placeholder for data quality: 100% reporting\n",
    "    quality_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"reporting_continuity_pct\": [100.0]*len(district_names),\n",
    "    })\n",
    "    quality_df[\"Data_quality_flag\"] = quality_df[\"reporting_continuity_pct\"].apply(classify_data_quality)\n",
    "\n",
    "    peak_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Peak_lead_time_weeks\": peak_leadtime_list,\n",
    "        \"Peak_cases_median\":    np.round(peak_val_list,2),\n",
    "        \"Peak_cases_high(p90)\": np.round(peak_hi_list,2),\n",
    "        \"Peak_when\":            peak_day_list\n",
    "    })\n",
    "\n",
    "    isochrone_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Forecast_next_week_median\": np.round(next_week_forecast_list,2),\n",
    "        \"Forecast_next_week_hi\":     np.round(next_week_hi_list,2),\n",
    "        \"Growth_flag\": [\n",
    "            classify_growth(g) for g in growth_rate_list\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    gap_pct_list = []\n",
    "    for a, p in zip(this_week_actual_list, this_week_pred_list):\n",
    "        gap_pct = (p - a)/(a + 1e-6)*100.0\n",
    "        gap_pct_list.append(gap_pct)\n",
    "    nowcast_gap_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Upazila\":  upazila_names,\n",
    "        \"Year\":     year_list,\n",
    "        \"Month\":    month_list,\n",
    "        \"Current_week_actual\":               np.round(this_week_actual_list,2),\n",
    "        \"Current_week_predicted_from_prev\":  np.round(this_week_pred_list,2),\n",
    "        \"Nowcast_gap_percent\":               np.round(gap_pct_list,1),\n",
    "    })\n",
    "\n",
    "    # climate importance via corr with forecast\n",
    "    last_feats_matrix = Xseq[:, -2, :-1]  # exclude prev_y channel\n",
    "    preds_arr = np.array(next_week_forecast_list)\n",
    "\n",
    "    feat_import_rows = []\n",
    "    def parse_feature_name(raw_name: str):\n",
    "        if raw_name.startswith(\"month_\"):\n",
    "            return \"season(month)\", \"\"\n",
    "        if \"_rmean\" in raw_name:\n",
    "            base, tail = raw_name.split(\"_rmean\")\n",
    "            return base, f\"{tail}m rolling mean\"\n",
    "        if \"_rstd\" in raw_name:\n",
    "            base, tail = raw_name.split(\"_rstd\")\n",
    "            return base, f\"{tail}m rolling std\"\n",
    "        if \"_lag\" in raw_name:\n",
    "            base, tail = raw_name.split(\"_lag\")\n",
    "            return base, f\"lag {tail}m\"\n",
    "        return raw_name, \"same month\"\n",
    "\n",
    "    for fi in range(last_feats_matrix.shape[1]):\n",
    "        col_vals = last_feats_matrix[:, fi]\n",
    "        if np.std(col_vals) < 1e-8:\n",
    "            corr = 0.0\n",
    "        else:\n",
    "            corr = np.corrcoef(col_vals, preds_arr)[0,1]\n",
    "\n",
    "        raw_feat_name = feat_cols[fi]\n",
    "        base_var, lag_info = parse_feature_name(raw_feat_name)\n",
    "        feat_import_rows.append({\n",
    "            \"feature\": raw_feat_name,\n",
    "            \"base_var\": base_var,\n",
    "            \"lag_info\": lag_info,\n",
    "            \"pearson_corr_with_next_week_forecast\": float(corr),\n",
    "        })\n",
    "\n",
    "    climate_influence_df = pd.DataFrame(feat_import_rows)\n",
    "    climate_influence_df[\"abs_corr\"] = (\n",
    "        climate_influence_df[\"pearson_corr_with_next_week_forecast\"].abs()\n",
    "    )\n",
    "    climate_influence_df = climate_influence_df.sort_values(\n",
    "        \"abs_corr\", ascending=False\n",
    "    )\n",
    "\n",
    "    top5 = watchlist_df.sort_values(\"High_scenario_p90\", ascending=False).head(5)[\n",
    "        [\"District\",\"Upazila\",\"Expected_cases_next_week\",\"High_scenario_p90\",\"Status\"]\n",
    "    ].to_dict(orient=\"records\")\n",
    "\n",
    "    rapid = accel_df[accel_df[\"Growth_flag\"]==\"🔴 Rapid growth\"][[\"District\",\"Upazila\"]].apply(tuple,1).tolist()\n",
    "    overflow_risk = overflow_df[overflow_df[\"Breach_risk_flag\"]==\"YES\"][[\"District\",\"Upazila\"]].apply(tuple,1).tolist()\n",
    "    dq_bad = quality_df[quality_df[\"Data_quality_flag\"]==\"🔴 Needs field check\"][[\"District\",\"Upazila\"]].apply(tuple,1).tolist()\n",
    "\n",
    "    summary_text = {\n",
    "        \"Top5_high_risk_next_week\": to_python(top5),\n",
    "        \"Upazilas_with_rapid_growth\": to_python(rapid),\n",
    "        \"Upazilas_with_capacity_breach_risk\": to_python(overflow_risk),\n",
    "        \"Upazilas_with_data_quality_issues\": to_python(dq_bad),\n",
    "        \"Model_calibration\": {\n",
    "            \"Coverage90\": round(float(overall_metrics[\"Coverage90\"]),3),\n",
    "            \"SMAPE\": round(float(overall_metrics[\"SMAPE\"]),2),\n",
    "            \"R2\": round(float(overall_metrics[\"R2\"]),3),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    products = {\n",
    "        \"watchlist_df\": watchlist_df,\n",
    "        \"overflow_df\": overflow_df,\n",
    "        \"accel_df\": accel_df,\n",
    "        \"confidence_df\": confidence_df,\n",
    "        \"quality_df\": quality_df,\n",
    "        \"peak_df\": peak_df,\n",
    "        \"isochrone_df\": isochrone_df,\n",
    "        \"nowcast_gap_df\": nowcast_gap_df,\n",
    "        \"climate_influence_df\": climate_influence_df,\n",
    "        \"executive_summary\": summary_text,\n",
    "        \"overall_metrics\": overall_metrics,\n",
    "        \"last_meta\": last_meta,\n",
    "    }\n",
    "\n",
    "    return overall_metrics, products\n",
    "\n",
    "# ============================================================\n",
    "# PUSH TO DB FOR MALARIA\n",
    "# ============================================================\n",
    "\n",
    "def push_products_to_db_malaria(conn, prefix, products):\n",
    "    last_meta = products[\"last_meta\"]\n",
    "\n",
    "    # watchlist\n",
    "    watchlist_rows = []\n",
    "    for _, r in products[\"watchlist_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        watchlist_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Expected_cases_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Status\"]\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_watchlist\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"expected_cases_next_week\",\"high_scenario_p90\",\"status\"],\n",
    "        watchlist_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # overflow\n",
    "    overflow_rows = []\n",
    "    for _, r in products[\"overflow_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        overflow_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            int(r[\"Capacity_threshold_beds_per_week\"]) if not pd.isna(r[\"Capacity_threshold_beds_per_week\"]) else None,\n",
    "            py(r[\"Forecast_median_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Breach_risk_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_overflow_risk\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"capacity_threshold_beds_per_week\",\n",
    "         \"forecast_median_next_week\",\"high_scenario_p90\",\"breach_risk_flag\"],\n",
    "        overflow_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # accel\n",
    "    accel_rows = []\n",
    "    for _, r in products[\"accel_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        accel_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Last_week_cases\"]),\n",
    "            py(r[\"This_week_actual\"]),\n",
    "            py(r[\"This_week_predicted\"]),\n",
    "            py(r[\"Next_week_forecast\"]),\n",
    "            py(r[\"Growth_rate_WoW(%)\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_acceleration_alerts\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"last_week_cases\",\"this_week_actual\",\"this_week_predicted\",\n",
    "         \"next_week_forecast\",\"growth_rate_wow\",\"growth_flag\"],\n",
    "        accel_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # confidence\n",
    "    conf_rows = []\n",
    "    for _, r in products[\"confidence_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        conf_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Forecast_next_week\"]),\n",
    "            py(r[\"Uncertainty_width(p90-p10)\"]),\n",
    "            r[\"Confidence_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_confidence\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"forecast_next_week\",\"uncertainty_width\",\"confidence_flag\"],\n",
    "        conf_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # quality\n",
    "    qual_rows = []\n",
    "    for _, r in products[\"quality_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        qual_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"reporting_continuity_pct\"]),\n",
    "            r[\"Data_quality_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_surveillance_quality\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"reporting_continuity_pct\",\"data_quality_flag\"],\n",
    "        qual_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # peak projection\n",
    "    peak_rows = []\n",
    "    for _, r in products[\"peak_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        peak_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Peak_lead_time_weeks\"]),\n",
    "            py(r[\"Peak_cases_median\"]),\n",
    "            py(r[\"Peak_cases_high(p90)\"]),\n",
    "            r[\"Peak_when\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_peak_projection\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"peak_lead_time_weeks\",\"peak_cases_median\",\n",
    "         \"peak_cases_high_p90\",\"peak_when\"],\n",
    "        peak_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # isochrone spread\n",
    "    iso_rows = []\n",
    "    for _, r in products[\"isochrone_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        iso_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Forecast_next_week_median\"]),\n",
    "            py(r[\"Forecast_next_week_hi\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_isochrone_spread\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"forecast_next_week_median\",\n",
    "         \"forecast_next_week_hi\",\"growth_flag\"],\n",
    "        iso_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # nowcast gap\n",
    "    gap_rows = []\n",
    "    for _, r in products[\"nowcast_gap_df\"].iterrows():\n",
    "        key = (r[\"District\"], r[\"Upazila\"])\n",
    "        meta = last_meta.get(key, {})\n",
    "        gap_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Upazila\"],\n",
    "            py(meta.get(\"Year\")),\n",
    "            py(meta.get(\"Month\")),\n",
    "            py(r[\"Current_week_actual\"]),\n",
    "            py(r[\"Current_week_predicted_from_prev\"]),\n",
    "            py(r[\"Nowcast_gap_percent\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_nowcast_gap\",\n",
    "        [\"district\",\"upazila\",\"year\",\"month\",\n",
    "         \"current_week_actual\",\n",
    "         \"current_week_predicted_from_prev\",\n",
    "         \"nowcast_gap_percent\"],\n",
    "        gap_rows,\n",
    "        pk_cols=[\"district\",\"upazila\",\"year\",\"month\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # climate influence\n",
    "    clim_rows = []\n",
    "    for _, r in products[\"climate_influence_df\"].iterrows():\n",
    "        clim_rows.append((\n",
    "            r[\"feature\"],\n",
    "            r[\"base_var\"],\n",
    "            r[\"lag_info\"],\n",
    "            py(r[\"pearson_corr_with_next_week_forecast\"]),\n",
    "            py(r[\"abs_corr\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_climate_influence\",\n",
    "        [\"feature\",\"base_var\",\"lag_info\",\n",
    "         \"pearson_corr_with_next_week_forecast\",\"abs_corr\"],\n",
    "        clim_rows,\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "    # exec summary\n",
    "    summary_json = json.dumps(to_python(products[\"executive_summary\"]))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        f\"{prefix}_exec_summary\",\n",
    "        [\"summary\"],\n",
    "        [(summary_json,)],\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE PER TARGET (pv_rate_1k / pf_rate_1k)\n",
    "# ============================================================\n",
    "\n",
    "def run_target_pipeline(df_all, target_col, prefix, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"\\n=== Running malaria target: {target_col} ({prefix}) ===\")\n",
    "\n",
    "    # Build lagged/rolled features on climate\n",
    "    df_feat, lag_cols = add_lags_rolls_monthly(\n",
    "        df_all,\n",
    "        group_col=\"upazilaid\",\n",
    "        base_cols=BASE_FEATURES,\n",
    "        lags=(1,3,6),\n",
    "        rolls=(3,6)\n",
    "    )\n",
    "\n",
    "    # final feature list\n",
    "    feat_cols = (\n",
    "        [\"month_sin\",\"month_cos\"] +\n",
    "        lag_cols\n",
    "    )\n",
    "\n",
    "    # per-upazila temporal split: TRAIN / VAL / TEST\n",
    "    tr_parts, va_parts, te_parts = [], [], []\n",
    "    for uid, g in df_feat.groupby(\"upazilaid\"):\n",
    "        g = g.sort_values([\"year\",\"month\"]).copy()\n",
    "        if len(g) > (VAL_H_MONTHS + TEST_H_MONTHS):\n",
    "            te_parts.append(g.iloc[-TEST_H_MONTHS:])\n",
    "            va_parts.append(g.iloc[-(VAL_H_MONTHS + TEST_H_MONTHS):-TEST_H_MONTHS])\n",
    "            tr_parts.append(g.iloc[:-(VAL_H_MONTHS + TEST_H_MONTHS)])\n",
    "        elif len(g) > TEST_H_MONTHS:\n",
    "            te_parts.append(g.iloc[-TEST_H_MONTHS:])\n",
    "            tr_parts.append(g.iloc[:-TEST_H_MONTHS])\n",
    "        else:\n",
    "            tr_parts.append(g)\n",
    "    tr_df = pd.concat(tr_parts).reset_index(drop=True)\n",
    "    va_df = pd.concat(va_parts).reset_index(drop=True) if va_parts else tr_df.iloc[0:0].copy()\n",
    "    te_df = pd.concat(te_parts).reset_index(drop=True) if te_parts else tr_df.iloc[0:0].copy()\n",
    "\n",
    "    # Fit scalers on TRAIN\n",
    "    split_tr, le_tr, feat_scaler_tr, y_scalers_tr = build_mats(tr_df, feat_cols, target_col)\n",
    "\n",
    "    # Sequences\n",
    "    X_tr, Y_tr, C_tr = to_seq(split_tr, SEQ, prev_y=True)\n",
    "    if X_tr.shape[0] == 0:\n",
    "        raise RuntimeError(\"No train sequences. Reduce SEQ or check data continuity.\")\n",
    "    print(f\"Train sequences: {X_tr.shape}\")\n",
    "\n",
    "    # Validation seqs\n",
    "    if len(va_df) > 0:\n",
    "        split_va = apply_mats(va_df, feat_cols, target_col, le_tr, feat_scaler_tr, y_scalers_tr)\n",
    "        X_va, Y_va, C_va = to_seq(split_va, SEQ, prev_y=True)\n",
    "        if X_va.shape[0] == 0:\n",
    "            X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "    else:\n",
    "        X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "\n",
    "    # Test seqs\n",
    "    if len(te_df) > 0:\n",
    "        split_te = apply_mats(te_df, feat_cols, target_col, le_tr, feat_scaler_tr, y_scalers_tr)\n",
    "        X_te, Y_te, C_te = to_seq(split_te, SEQ, prev_y=True)\n",
    "        if X_te.shape[0] == 0:\n",
    "            X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "    else:\n",
    "        X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "\n",
    "    # Build lookup df for metadata per entity_id for TEST split\n",
    "    # We want each upazila's rows in time order, including names + year/month\n",
    "    te_df_with_eid = te_df.copy()\n",
    "    te_df_with_eid[\"entity_id\"] = le_tr.transform(te_df_with_eid[\"upazilaid\"].astype(str))\n",
    "    seq_lookup_df = {}\n",
    "    for cid, g in te_df_with_eid.groupby(\"entity_id\"):\n",
    "        g = g.sort_values([\"year\",\"month\"]).copy()\n",
    "        seq_lookup_df[int(cid)] = g\n",
    "\n",
    "    # Train model\n",
    "    nc = int(C_tr.max()) + 1\n",
    "    cond_dim = X_tr.shape[2]\n",
    "    print(\"Training LSTM+Attention forecaster ...\")\n",
    "    M = train_model(\n",
    "        X_tr, Y_tr, C_tr,\n",
    "        X_va, Y_va, C_va,\n",
    "        cond_dim, nc\n",
    "    )\n",
    "    torch.save(M.state_dict(), os.path.join(out_dir, \"forecaster.pt\"))\n",
    "\n",
    "    # Build director products on TEST split\n",
    "    overall_metrics, products = evaluate_and_build_products_malaria(\n",
    "        M,\n",
    "        X_te, Y_te, C_te,\n",
    "        y_scalers_tr, le_tr,\n",
    "        seq_lookup_df,\n",
    "        feat_cols\n",
    "    )\n",
    "\n",
    "    # save local artifacts\n",
    "    products[\"watchlist_df\"].to_csv(os.path.join(out_dir,\"watchlist.csv\"), index=False)\n",
    "    products[\"overflow_df\"].to_csv(os.path.join(out_dir,\"overflow_risk.csv\"), index=False)\n",
    "    products[\"accel_df\"].to_csv(os.path.join(out_dir,\"acceleration_alerts.csv\"), index=False)\n",
    "    products[\"confidence_df\"].to_csv(os.path.join(out_dir,\"forecast_confidence.csv\"), index=False)\n",
    "    products[\"quality_df\"].to_csv(os.path.join(out_dir,\"surveillance_quality.csv\"), index=False)\n",
    "    products[\"peak_df\"].to_csv(os.path.join(out_dir,\"peak_projection.csv\"), index=False)\n",
    "    products[\"isochrone_df\"].to_csv(os.path.join(out_dir,\"isochrone_spread.csv\"), index=False)\n",
    "    products[\"nowcast_gap_df\"].to_csv(os.path.join(out_dir,\"nowcast_gap.csv\"), index=False)\n",
    "    products[\"climate_influence_df\"].to_csv(os.path.join(out_dir,\"climate_lag_influence.csv\"), index=False)\n",
    "\n",
    "    dash_manifest = {\n",
    "        \"executive_summary\": products[\"executive_summary\"],\n",
    "        \"overall_metrics\": products[\"overall_metrics\"],\n",
    "        \"notes\": f\"Auto-generated malaria decision tables for {prefix} (monthly model scaled to 'weekly').\"\n",
    "    }\n",
    "    with open(os.path.join(out_dir,\"DASHBOARD_SUMMARY.json\"), \"w\") as f:\n",
    "        json.dump(to_python(dash_manifest), f, indent=2)\n",
    "\n",
    "    # Push to DB\n",
    "    conn = get_db_conn()\n",
    "    ensure_output_tables_malaria(conn, prefix)\n",
    "    push_products_to_db_malaria(conn, prefix, products)\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n=== Executive Summary Preview ===\")\n",
    "    print(json.dumps(to_python(dash_manifest[\"executive_summary\"]), indent=2))\n",
    "    print(f\"\\nArtifacts saved to {out_dir} and pushed to Postgres.\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"1) Load malaria monthly data from Postgres\")\n",
    "    df_all = load_malaria_monthly_from_db()\n",
    "\n",
    "    # PV\n",
    "    pv_out_dir = os.path.join(OUT_ROOT, \"pv_rate_1k\")\n",
    "    run_target_pipeline(\n",
    "        df_all,\n",
    "        target_col=\"pv_rate_1k\",\n",
    "        prefix=\"malaria_pv\",              # tables: malaria_pv_*\n",
    "        out_dir=pv_out_dir\n",
    "    )\n",
    "\n",
    "    # PF\n",
    "    pf_out_dir = os.path.join(OUT_ROOT, \"pf_rate_1k\")\n",
    "    run_target_pipeline(\n",
    "        df_all,\n",
    "        target_col=\"pf_rate_1k\",\n",
    "        prefix=\"malaria_pf\",              # tables: malaria_pf_*\n",
    "        out_dir=pf_out_dir\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
