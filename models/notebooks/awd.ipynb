{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7dc33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# =====================================================================================\n",
    "# CONFIG / CONSTANTS\n",
    "# =====================================================================================\n",
    "\n",
    "PG_HOST    = \"119.148.17.102\"\n",
    "PG_PORT    = 5432\n",
    "PG_DB      = \"ewarsdb\"\n",
    "PG_USER    = \"ewars\"\n",
    "PG_PASS    = \"Iedcr@Ewars2025\"\n",
    "PG_SSLMODE = \"require\"\n",
    "\n",
    "OUT = \"/content/diarrhoea_out\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "SEED=42\n",
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# sequence/training\n",
    "SEQ=30              # lookback window length (days)\n",
    "BATCH=128\n",
    "EPOCHS=300\n",
    "PATIENCE=25\n",
    "LSTM_UNITS=160\n",
    "HEADS=8\n",
    "DROP=0.2\n",
    "LR=8e-4\n",
    "WD=1e-4\n",
    "CLIP=1.0\n",
    "TF_START=1.0\n",
    "TF_END=0.35\n",
    "Q=(0.1,0.5,0.9)\n",
    "\n",
    "# forecast horizon for per-district roll-forward (days)\n",
    "HORIZON=15\n",
    "\n",
    "# feature engineering params\n",
    "CASE_LAGS = (1,2,3,7,14,21,28)\n",
    "WEA_LAGS  = (1,2,3,7)\n",
    "CASE_ROLL = (7,14)\n",
    "WEA_ROLL  = (7,)\n",
    "\n",
    "# director table logic / thresholds (reuse dengue style)\n",
    "DEFAULT_CAPACITY_PER_DISTRICT = 25\n",
    "RAPID_GROWTH_THRESH = 0.30       # >=30% wow growth => red\n",
    "MODERATE_GROWTH_THRESH = 0.10    # >=10% wow growth => orange\n",
    "\n",
    "PEAK_LOOKAHEAD_DAYS = 15         # look-ahead window for \"peak projection\" (use horizon)\n",
    "CONF_HIGH = 5        # width<=5 => high confidence\n",
    "CONF_MED  = 15       # width<=15 => medium\n",
    "\n",
    "# random seeding for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# PG HELPERS\n",
    "# =====================================================================================\n",
    "\n",
    "def get_db_conn():\n",
    "    dsn = (\n",
    "        f\"host={PG_HOST} port={PG_PORT} dbname={PG_DB} \"\n",
    "        f\"user={PG_USER} password={PG_PASS} sslmode={PG_SSLMODE}\"\n",
    "    )\n",
    "    return psycopg2.connect(dsn)\n",
    "\n",
    "def to_python(obj):\n",
    "    if isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        val = float(obj)\n",
    "        if math.isnan(val):\n",
    "            return None\n",
    "        return val\n",
    "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return [to_python(x) for x in obj.tolist()]\n",
    "\n",
    "    if isinstance(obj, (pd.Timestamp,)):\n",
    "        if pd.isna(obj):\n",
    "            return None\n",
    "        return obj.isoformat()\n",
    "\n",
    "    if isinstance(obj, float):\n",
    "        if math.isnan(obj):\n",
    "            return None\n",
    "        return obj\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_python(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [to_python(v) for v in obj]\n",
    "\n",
    "    return obj\n",
    "\n",
    "def py(v):\n",
    "    if isinstance(v, (np.floating, np.float32, np.float64)):\n",
    "        v = float(v)\n",
    "    elif isinstance(v, (np.integer, np.int32, np.int64)):\n",
    "        v = int(v)\n",
    "    elif isinstance(v, (np.bool_,)):\n",
    "        v = bool(v)\n",
    "    if isinstance(v, float) and (math.isnan(v)):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "def ensure_output_tables_diarrhoea(conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # identical schemas to dengue_* but prefixed diarrhoea_*\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_watchlist (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        expected_cases_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        status TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_overflow_risk (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        capacity_threshold_beds_per_week INT,\n",
    "        forecast_median_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        breach_risk_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_acceleration_alerts (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        last_week_cases NUMERIC,\n",
    "        this_week_actual NUMERIC,\n",
    "        this_week_predicted NUMERIC,\n",
    "        next_week_forecast NUMERIC,\n",
    "        growth_rate_wow NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_confidence (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        forecast_next_week NUMERIC,\n",
    "        uncertainty_width NUMERIC,\n",
    "        confidence_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_surveillance_quality (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        reporting_continuity_pct NUMERIC,\n",
    "        data_quality_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_peak_projection (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        peak_lead_time_weeks NUMERIC,\n",
    "        peak_cases_median NUMERIC,\n",
    "        peak_cases_high_p90 NUMERIC,\n",
    "        peak_when TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_isochrone_spread (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        forecast_next_week_median NUMERIC,\n",
    "        forecast_next_week_hi NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_nowcast_gap (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        current_week_actual NUMERIC,\n",
    "        current_week_predicted_from_prev NUMERIC,\n",
    "        nowcast_gap_percent NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_climate_influence (\n",
    "        feature TEXT,\n",
    "        base_var TEXT,\n",
    "        lag_info TEXT,\n",
    "        pearson_corr_with_next_week_forecast NUMERIC,\n",
    "        abs_corr NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diarrhoea_exec_summary (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        summary JSONB,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "def _dedupe_by_pk(rows, cols, pk_cols):\n",
    "    if not pk_cols:\n",
    "        return rows\n",
    "    pk_idx = [cols.index(pk) for pk in pk_cols]\n",
    "    dedup = {}\n",
    "    for row in rows:\n",
    "        key = tuple(row[i] for i in pk_idx)\n",
    "        dedup[key] = row\n",
    "    return list(dedup.values())\n",
    "\n",
    "def upsert_table(conn, table_name, cols, rows, pk_cols=None, wipe_first=False):\n",
    "    if wipe_first:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"TRUNCATE TABLE {table_name};\")\n",
    "        conn.commit()\n",
    "    if not rows:\n",
    "        return\n",
    "    if not pk_cols:\n",
    "        with conn.cursor() as cur:\n",
    "            insert_sql = f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s\"\n",
    "            execute_values(cur, insert_sql, rows)\n",
    "        conn.commit()\n",
    "        return\n",
    "\n",
    "    rows_dedup = _dedupe_by_pk(rows, cols, pk_cols)\n",
    "    conflict_target = \", \".join(pk_cols)\n",
    "    set_updates = \", \".join([f\"{c}=EXCLUDED.{c}\" for c in cols if c not in pk_cols])\n",
    "    insert_sql = (\n",
    "        f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s \"\n",
    "        f\"ON CONFLICT ({conflict_target}) DO UPDATE SET {set_updates};\"\n",
    "    )\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, insert_sql, rows_dedup)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# LOAD / PREP DAILY DATA FROM DB\n",
    "# =====================================================================================\n",
    "\n",
    "REQ = [\"division\",\"district\",\"date\",\"daily_cases\",\"temperature\",\"humidity\",\"rainfall\"]\n",
    "\n",
    "def load_daily_from_db():\n",
    "    \"\"\"\n",
    "    Pull raw daily diarrhoea + weather from Postgres (awd_weather),\n",
    "    aggregate duplicates per district/date, pad missing days per district,\n",
    "    fill exogenous (ffill/bfill), cases missing -> 0, add time features.\n",
    "    \"\"\"\n",
    "    conn = get_db_conn()\n",
    "    try:\n",
    "        sql = \"\"\"\n",
    "            SELECT\n",
    "                division,\n",
    "                district,\n",
    "                date,\n",
    "                daily_cases,\n",
    "                temperature,\n",
    "                humidity,\n",
    "                rainfall\n",
    "            FROM awd_weather\n",
    "            WHERE date IS NOT NULL\n",
    "            ORDER BY district, date;\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(sql, conn)\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    # normalize cols\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # sanity\n",
    "    missing = [c for c in REQ if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"DB missing columns {missing}\")\n",
    "\n",
    "    # types\n",
    "    for cat in [\"division\",\"district\"]:\n",
    "        df[cat] = df[cat].astype(str).str.strip()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "    for c in [\"daily_cases\",\"temperature\",\"humidity\",\"rainfall\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # aggregate duplicates (sum cases, mean weather)\n",
    "    grp = df.groupby([\"division\",\"district\",\"date\"], as_index=False)\n",
    "    df = grp.agg({\n",
    "        \"daily_cases\":\"sum\",\n",
    "        \"temperature\":\"mean\",\n",
    "        \"humidity\":\"mean\",\n",
    "        \"rainfall\":\"mean\",\n",
    "    })\n",
    "\n",
    "    # pad per district to continuous daily timeline\n",
    "    parts = []\n",
    "    for (div, dis), g in df.groupby([\"division\",\"district\"], sort=False):\n",
    "        g = g.sort_values(\"date\")\n",
    "        if len(g)==0:\n",
    "            continue\n",
    "        full_idx = pd.date_range(g[\"date\"].min(), g[\"date\"].max(), freq=\"D\")\n",
    "        g = g.set_index(\"date\").reindex(full_idx)\n",
    "        g.index.name = \"date\"\n",
    "        g = g.reset_index()\n",
    "        g[\"division\"] = div\n",
    "        g[\"district\"] = dis\n",
    "\n",
    "        # fill exogenous forward/back\n",
    "        for c in [\"temperature\",\"humidity\",\"rainfall\"]:\n",
    "            g[c] = g[c].ffill().bfill()\n",
    "\n",
    "        # missing cases -> 0\n",
    "        g[\"daily_cases\"] = g[\"daily_cases\"].fillna(0.0)\n",
    "\n",
    "        parts.append(g)\n",
    "\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # time features\n",
    "    df[\"doy\"] = df[\"date\"].dt.dayofyear.astype(int)         # 1..366\n",
    "    df[\"dow\"] = df[\"date\"].dt.dayofweek.astype(int)         # 0..6\n",
    "    df[\"doy_sin\"] = np.sin(2*np.pi*df[\"doy\"].astype(float)/366.0).astype(np.float32)\n",
    "    df[\"doy_cos\"] = np.cos(2*np.pi*df[\"doy\"].astype(float)/366.0).astype(np.float32)\n",
    "    df[\"dow_sin\"] = np.sin(2*np.pi*df[\"dow\"].astype(float)/7.0).astype(np.float32)\n",
    "    df[\"dow_cos\"] = np.cos(2*np.pi*df[\"dow\"].astype(float)/7.0).astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# FEATURE ENGINEERING (LAGS / ROLLS)\n",
    "# =====================================================================================\n",
    "\n",
    "def add_lags_rolls_daily(df):\n",
    "    df = df.sort_values([\"district\",\"date\"]).copy()\n",
    "    new_cols = []\n",
    "\n",
    "    # case lags/rolls\n",
    "    for L in CASE_LAGS:\n",
    "        col = f\"daily_cases_lag{L}\"\n",
    "        df[col] = df.groupby(\"district\")[\"daily_cases\"].shift(L)\n",
    "        new_cols.append(col)\n",
    "\n",
    "    for R in CASE_ROLL:\n",
    "        m = f\"daily_cases_rmean{R}\"\n",
    "        s = f\"daily_cases_rstd{R}\"\n",
    "        g = df.groupby(\"district\")[\"daily_cases\"]\n",
    "        df[m] = g.rolling(R, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        df[s] = g.rolling(R, min_periods=1).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "        new_cols += [m,s]\n",
    "\n",
    "    # weather lags/rolls\n",
    "    for w in [\"temperature\",\"humidity\",\"rainfall\"]:\n",
    "        for L in WEA_LAGS:\n",
    "            col = f\"{w}_lag{L}\"\n",
    "            df[col] = df.groupby(\"district\")[w].shift(L)\n",
    "            new_cols.append(col)\n",
    "\n",
    "        for R in WEA_ROLL:\n",
    "            m = f\"{w}_rmean{R}\"\n",
    "            g = df.groupby(\"district\")[w]\n",
    "            df[m] = g.rolling(R, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            new_cols.append(m)\n",
    "\n",
    "    df[new_cols] = df[new_cols].fillna(0.0)\n",
    "    return df, new_cols\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# MATRIX BUILDERS / DATASETS\n",
    "# =====================================================================================\n",
    "\n",
    "class Split:\n",
    "    def __init__(self, X, y, c, doy, dow, df):\n",
    "        self.X=X; self.y=y; self.c=c; self.doy=doy; self.dow=dow; self.df=df\n",
    "\n",
    "def build_mats(df, feat, target=\"daily_cases\"):\n",
    "    le = LabelEncoder().fit(df[\"district\"].values)\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"district_id\"] = le.transform(d[\"district\"])\n",
    "\n",
    "    X = d[feat].values.astype(np.float32)\n",
    "    feat_scaler = StandardScaler().fit(X)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y = np.log1p(np.clip(d[target].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "    y_scalers = {}\n",
    "    for cid, g in d.groupby(\"district_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = StandardScaler().fit(y[idx])\n",
    "        y_scalers[cid] = sc\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    doy = (d[\"doy\"].values.astype(int)-1).astype(np.int64)  # 0..365\n",
    "    dow = d[\"dow\"].values.astype(np.int64)                  # 0..6\n",
    "    return Split(Xs, Ys, d[\"district_id\"].values.astype(np.int64), doy, dow, d), le, feat_scaler, y_scalers\n",
    "\n",
    "def apply_mats(df, feat, target, le, feat_scaler, y_scalers):\n",
    "    d = df.copy()\n",
    "    d[\"district_id\"] = le.transform(d[\"district\"])\n",
    "\n",
    "    X = d[feat].values.astype(np.float32)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y = np.log1p(np.clip(d[target].values.reshape(-1,1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "    for cid, g in d.groupby(\"district_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = y_scalers.get(int(cid), StandardScaler().fit(y[idx]))\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    doy = (d[\"doy\"].values.astype(int)-1).astype(np.int64)\n",
    "    dow = d[\"dow\"].values.astype(np.int64)\n",
    "    return Split(Xs, Ys, d[\"district_id\"].values.astype(np.int64), doy, dow, d)\n",
    "\n",
    "def to_seq(split, L=SEQ, prev_y=True):\n",
    "    X,y,c,df = split.X, split.y, split.c, split.df\n",
    "    doy_idx, dow_idx = split.doy, split.dow\n",
    "\n",
    "    # ordinal index for \"consecutive days\" check\n",
    "    ord_idx = split.df[\"date\"].values.astype(\"datetime64[D]\").astype(np.int64)\n",
    "\n",
    "    SX,SY,SC,SDoY,SDow = [],[],[],[],[]\n",
    "    for cid in np.unique(c):\n",
    "        idx = np.where(c==cid)[0]\n",
    "        order = np.argsort(ord_idx[idx])\n",
    "        idx = idx[order]\n",
    "        o   = ord_idx[idx]\n",
    "        for i in range(len(idx)-L+1):\n",
    "            sl = idx[i:i+L]\n",
    "            # require strictly consecutive dates\n",
    "            if np.all(np.diff(o[i:i+L]) == 1):\n",
    "                Xi = X[sl]\n",
    "                Yi = y[sl]\n",
    "                if prev_y:\n",
    "                    prev = np.vstack([np.zeros((1,1),np.float32), Yi[:-1]])\n",
    "                    Xi = np.concatenate([Xi, prev], axis=1)\n",
    "                SX.append(Xi)\n",
    "                SY.append(Yi)\n",
    "                SC.append(cid)\n",
    "                SDoY.append(doy_idx[sl])\n",
    "                SDow.append(dow_idx[sl])\n",
    "\n",
    "    return (\n",
    "        np.asarray(SX, np.float32),\n",
    "        np.asarray(SY, np.float32),\n",
    "        np.asarray(SC, np.int64),\n",
    "        np.asarray(SDoY, np.int64),\n",
    "        np.asarray(SDow, np.int64)\n",
    "    )\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# MODEL\n",
    "# =====================================================================================\n",
    "\n",
    "class Forecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM + causal self-attention with district/time embeddings;\n",
    "    outputs mean, log-sigma, and quantiles.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_dim, n_district,\n",
    "                 emb_dist=16, emb_doy=12, emb_dow=6,\n",
    "                 lstm=LSTM_UNITS, heads=HEADS, drop=DROP, qu=Q):\n",
    "        super().__init__()\n",
    "        self.q = qu\n",
    "        self.ed = nn.Embedding(n_district, emb_dist)\n",
    "        self.et_doy = nn.Embedding(366, emb_doy)\n",
    "        self.et_dow = nn.Embedding(7,   emb_dow)\n",
    "        in_dim = cond_dim + emb_dist + emb_doy + emb_dow\n",
    "\n",
    "        self.lstm = nn.LSTM(in_dim, lstm, 1, batch_first=True)\n",
    "        self.mha  = nn.MultiheadAttention(lstm, heads, batch_first=True, dropout=drop)\n",
    "        self.ln   = nn.LayerNorm(lstm)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "        self.mu = nn.Linear(lstm,1)\n",
    "        self.ls = nn.Linear(lstm,1)\n",
    "        self.qh = nn.ModuleList([nn.Linear(lstm,1) for _ in qu])\n",
    "\n",
    "        self.last_attn = None\n",
    "\n",
    "    def forward(self, cond, cid, doy, dow):\n",
    "        B,L,D = cond.shape\n",
    "        e_dist = self.ed(cid).unsqueeze(1).repeat(1,L,1)  # [B,L,emb_dist]\n",
    "        e_doy  = self.et_doy(doy)                         # [B,L,emb_doy]\n",
    "        e_dow  = self.et_dow(dow)                         # [B,L,emb_dow]\n",
    "        x = torch.cat([cond, e_dist, e_doy, e_dow], dim=-1)\n",
    "\n",
    "        h,_  = self.lstm(x)\n",
    "\n",
    "        # causal mask for attention\n",
    "        mask = torch.triu(torch.ones(L, L, device=h.device, dtype=torch.bool), diagonal=1)\n",
    "        att, w = self.mha(h,h,h,attn_mask=mask,need_weights=True)\n",
    "        self.last_attn = w.detach()\n",
    "\n",
    "        h = self.drop(self.ln(att))\n",
    "\n",
    "        mu = self.mu(h)\n",
    "        ls = torch.clamp(self.ls(h), -5.0, 3.0)\n",
    "        qs = [head(h) for head in self.qh]\n",
    "\n",
    "        return mu, ls, qs\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# TRAINING / VALIDATION\n",
    "# =====================================================================================\n",
    "\n",
    "def smape(y, p):\n",
    "    y = y.flatten()\n",
    "    p = p.flatten()\n",
    "    return 100*np.mean(2*np.abs(p-y)/(np.abs(y)+np.abs(p)+1e-8))\n",
    "\n",
    "def pinball(pred, target, quantile):\n",
    "    err = target - pred\n",
    "    return torch.mean(torch.maximum(quantile * err, (quantile - 1) * err))\n",
    "\n",
    "def tv_l1_on_mu(mu):\n",
    "    diff = mu[:,1:,:] - mu[:,:-1,:]\n",
    "    return diff.abs().mean(), mu.abs().mean()\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(self,X,Y,C,DoY,DoW):\n",
    "        self.X=X; self.Y=Y; self.C=C; self.DoY=DoY; self.DoW=DoW\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,i):\n",
    "        return self.X[i], self.Y[i], self.C[i], self.DoY[i], self.DoW[i]\n",
    "\n",
    "def validate_model(M,Xv,Yv,Cv,DvY,DvW):\n",
    "    M.eval()\n",
    "    with torch.no_grad():\n",
    "        X  = torch.tensor(Xv, dtype=torch.float32, device=DEVICE)\n",
    "        C  = torch.tensor(Cv, dtype=torch.long,   device=DEVICE)\n",
    "        DY = torch.tensor(DvY,dtype=torch.long,   device=DEVICE)\n",
    "        DW = torch.tensor(DvW,dtype=torch.long,   device=DEVICE)\n",
    "\n",
    "        mu, ls, qs = M(X, C, DY, DW)\n",
    "\n",
    "        mp  = mu[...,0].cpu().numpy()\n",
    "        y   = Yv[...,0]\n",
    "        q10 = qs[0][...,0].cpu().numpy()\n",
    "        q90 = qs[2][...,0].cpu().numpy()\n",
    "\n",
    "        coverage = float(np.mean((y>=q10) & (y<=q90)))\n",
    "        score    = smape(y.reshape(-1), mp.reshape(-1))\n",
    "\n",
    "    return score, coverage\n",
    "\n",
    "def train_model(Xtr,Ytr,Ctr,DtrY,DtrW,\n",
    "                Xva,Yva,Cva,DvaY,DvaW,\n",
    "                cond_dim,nc):\n",
    "    M = Forecaster(cond_dim=cond_dim, n_district=nc).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(M.parameters(),\n",
    "                            lr=LR,\n",
    "                            betas=(0.9,0.999),\n",
    "                            weight_decay=WD)\n",
    "\n",
    "    dl = DataLoader(SeqDS(Xtr,Ytr,Ctr,DtrY,DtrW),\n",
    "                    batch_size=BATCH,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "\n",
    "    best=float(\"inf\")\n",
    "    best_sd=None\n",
    "    wait=0\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        # scheduled sampling factor for prev_y channel\n",
    "        t  = e / max(1,(EPOCHS-1))\n",
    "        tf = TF_START + (TF_END-TF_START)*t\n",
    "\n",
    "        M.train()\n",
    "        for Xb,Yb,Cb,DYb,DWb in dl:\n",
    "            Xb  = Xb.to(torch.float32).to(DEVICE)\n",
    "            Yb  = Yb.to(torch.float32).to(DEVICE)\n",
    "            Cb  = Cb.to(torch.long).to(DEVICE)\n",
    "            DYb = DYb.to(torch.long).to(DEVICE)\n",
    "            DWb = DWb.to(torch.long).to(DEVICE)\n",
    "\n",
    "            B,L,_ = Xb.shape\n",
    "\n",
    "            # teacher forcing bleed for prev_y (last feature in X)\n",
    "            with torch.no_grad():\n",
    "                mu0,_,_ = M(Xb,Cb,DYb,DWb)\n",
    "                prev = torch.cat(\n",
    "                    [torch.zeros(B,1,device=DEVICE), mu0[:,:-1,0]],\n",
    "                    dim=1\n",
    "                )\n",
    "            Xb[:,:,-1] = tf*Xb[:,:,-1] + (1-tf)*prev\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            mu, ls, qs = M(Xb,Cb,DYb,DWb)\n",
    "\n",
    "            sig = (ls.exp()).clamp(1e-3,50.0)\n",
    "            nll = 0.5*(((Yb-mu)/sig)**2 + 2*ls + math.log(2*math.pi)).mean()\n",
    "\n",
    "            ql = 0.0\n",
    "            for i, qv in enumerate(Q):\n",
    "                ql += pinball(qs[i],Yb,qv)\n",
    "            ql /= len(Q)\n",
    "\n",
    "            tv, l1 = tv_l1_on_mu(mu)\n",
    "            loss = 1.0*nll + 1.0*ql + 0.05*tv + 0.02*l1\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(M.parameters(),CLIP)\n",
    "            opt.step()\n",
    "\n",
    "        sm, cov = validate_model(M,Xva,Yva,Cva,DvaY,DvaW)\n",
    "        comp = sm + 10*abs(cov-0.9)\n",
    "\n",
    "        if comp < best-1e-6:\n",
    "            best    = comp\n",
    "            best_sd = copy.deepcopy(M.state_dict())\n",
    "            wait    = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_sd is not None:\n",
    "        M.load_state_dict(best_sd)\n",
    "    return M\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# EVALUATION HELPERS FOR DIRECTOR TABLES\n",
    "# =====================================================================================\n",
    "\n",
    "def inverse_seq(arr_scaled, scaler):\n",
    "    # arr_scaled shape (L,) or (L,1)\n",
    "    arr_scaled = np.array(arr_scaled).reshape(-1,1)\n",
    "    unstd = scaler.inverse_transform(arr_scaled).reshape(-1)\n",
    "    return np.clip(np.expm1(unstd), 0, None)\n",
    "\n",
    "def classify_growth(g):\n",
    "    if g >= RAPID_GROWTH_THRESH:\n",
    "        return \"ðŸ”´ Rapid growth\"\n",
    "    elif g >= MODERATE_GROWTH_THRESH:\n",
    "        return \"ðŸŸ  Moderate growth\"\n",
    "    else:\n",
    "        return \"ðŸŸ¢ Stable/decline\"\n",
    "\n",
    "def classify_risk(high_scenario):\n",
    "    if high_scenario >= 40:\n",
    "        return \"ðŸ”´ Surge likely\"\n",
    "    elif high_scenario >= 20:\n",
    "        return \"ðŸŸ  Elevated\"\n",
    "    else:\n",
    "        return \"ðŸŸ¢ Low\"\n",
    "\n",
    "def classify_conf(width):\n",
    "    if width <= CONF_HIGH:\n",
    "        return \"âœ… High confidence\"\n",
    "    elif width <= CONF_MED:\n",
    "        return \"ðŸŸ¡ Medium confidence\"\n",
    "    else:\n",
    "        return \"âš  Low confidence\"\n",
    "\n",
    "def classify_data_quality(continuity_pct):\n",
    "    if continuity_pct >= 90:\n",
    "        return \"âœ… Reliable\"\n",
    "    elif continuity_pct >= 70:\n",
    "        return \"ðŸŸ¡ Watch\"\n",
    "    else:\n",
    "        return \"ðŸ”´ Needs field check\"\n",
    "\n",
    "def calc_reporting_continuity_daily(df_full_raw, lookback_days=SEQ):\n",
    "    \"\"\"\n",
    "    For each district, % of days in the last `lookback_days` that had any report.\n",
    "    daily_cases==0 counts as reported (we assume 0 means reported zero, not missing).\n",
    "    We'll treat NaN as not reported.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    for dist, g in df_full_raw.groupby(\"district\"):\n",
    "        g = g.sort_values(\"date\")\n",
    "        recent = g.tail(lookback_days)\n",
    "        # \"reported\" means not NaN\n",
    "        reported_mask = ~recent[\"daily_cases\"].isna()\n",
    "        continuity_pct = 100.0 * reported_mask.mean()\n",
    "        out_rows.append({\n",
    "            \"district\": dist,\n",
    "            f\"Reporting_Continuity_Last{lookback_days}d(%)\": continuity_pct\n",
    "        })\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "def evaluate_and_build_products(\n",
    "    M,\n",
    "    Xseq, Yseq, Cseq, DoYseq, DoWseq,\n",
    "    y_scalers, le, df_full_raw,\n",
    "    feat_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    Build director tables using last two forecast steps in each sequence:\n",
    "      - \"current day\" ~ second last day in sequence\n",
    "      - \"next day\"    ~ last day in sequence\n",
    "    We'll aggregate to pseudo-week by scaling these to 7d equivalence to match dengue-style tables.\n",
    "    The epi_week/year values come from ISO calendar of the last observed date per district.\n",
    "    \"\"\"\n",
    "\n",
    "    M.eval()\n",
    "\n",
    "    X_t = torch.tensor(Xseq, dtype=torch.float32, device=DEVICE)\n",
    "    C_t = torch.tensor(Cseq, dtype=torch.long, device=DEVICE)\n",
    "    DY_t= torch.tensor(DoYseq,dtype=torch.long, device=DEVICE)\n",
    "    DW_t= torch.tensor(DoWseq,dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, ls, qs = M(X_t, C_t, DY_t, DW_t)\n",
    "        mu_np  = mu.cpu().numpy()[...,0]      # [N,L]\n",
    "        q10_np = qs[0].cpu().numpy()[...,0]\n",
    "        q50_np = qs[1].cpu().numpy()[...,0]\n",
    "        q90_np = qs[2].cpu().numpy()[...,0]\n",
    "\n",
    "    # indexes\n",
    "    idx_curr = -2  # \"this period\"\n",
    "    idx_next = -1  # \"forecast next period\"\n",
    "\n",
    "    # we'll track per-district aggregates\n",
    "    district_names = []\n",
    "    last_actual_list = []\n",
    "    current_actual_list = []\n",
    "    current_pred_list = []\n",
    "    next_day_pred_list = []\n",
    "    next_day_hi_list = []\n",
    "    next_day_lo_list = []\n",
    "    growth_rate_list = []\n",
    "    conf_width_list = []\n",
    "    peak_day_list = []\n",
    "    peak_value_list = []\n",
    "    peak_hi_list = []\n",
    "    peak_leadtime_list = []\n",
    "    epiweek_meta_rows = []\n",
    "\n",
    "    # metrics arrays for global calibration\n",
    "    all_y_real = []\n",
    "    all_mu_real = []\n",
    "    all_lo_real = []\n",
    "    all_hi_real = []\n",
    "\n",
    "    # We'll also need last epi-week/year per district\n",
    "    last_epi_meta = {}\n",
    "\n",
    "    for i, cid in enumerate(Cseq):\n",
    "        sc = y_scalers[int(cid)]\n",
    "        dist_name = le.inverse_transform([cid])[0]\n",
    "\n",
    "        # inverse scale full sequence\n",
    "        y_real_full   = inverse_seq(Yseq[i].reshape(-1),        sc)\n",
    "        mu_real_full  = inverse_seq(mu_np[i].reshape(-1),       sc)\n",
    "        lo_real_full  = inverse_seq(q10_np[i].reshape(-1),      sc)\n",
    "        hi_real_full  = inverse_seq(q90_np[i].reshape(-1),      sc)\n",
    "\n",
    "        all_y_real.append(y_real_full)\n",
    "        all_mu_real.append(mu_real_full)\n",
    "        all_lo_real.append(lo_real_full)\n",
    "        all_hi_real.append(hi_real_full)\n",
    "\n",
    "        safe_idx_curr = idx_curr if idx_curr >= -len(y_real_full) else -1\n",
    "        safe_idx_next = idx_next if idx_next >= -len(y_real_full) else -1\n",
    "\n",
    "        # observations/preds\n",
    "        actual_curr  = y_real_full[safe_idx_curr]\n",
    "        pred_curr    = mu_real_full[safe_idx_curr]\n",
    "        actual_prev  = y_real_full[safe_idx_curr-1] if (safe_idx_curr-1)>=-len(y_real_full) else y_real_full[safe_idx_curr]\n",
    "\n",
    "        pred_next    = mu_real_full[safe_idx_next]\n",
    "        hi_next      = hi_real_full[safe_idx_next]\n",
    "        lo_next      = lo_real_full[safe_idx_next]\n",
    "\n",
    "        # growth % from prev actual to next predicted\n",
    "        growth = (pred_next - actual_prev) / (actual_prev + 1e-6)\n",
    "\n",
    "        # predictive band width\n",
    "        conf_width = hi_next - lo_next\n",
    "\n",
    "        # peak projection over the last PEAK_LOOKAHEAD_DAYS of mu_real_full\n",
    "        look_slice = mu_real_full[-PEAK_LOOKAHEAD_DAYS:]\n",
    "        hi_slice   = hi_real_full[-PEAK_LOOKAHEAD_DAYS:]\n",
    "        if len(look_slice)==0:\n",
    "            peak_when=\"NA\"; peak_val=np.nan; peak_hi=np.nan; lead_days=np.nan\n",
    "        else:\n",
    "            local_max_idx = int(np.argmax(look_slice))\n",
    "            peak_val = look_slice[local_max_idx]\n",
    "            peak_hi  = hi_slice[local_max_idx]\n",
    "            # lead_days from \"now\" (end)\n",
    "            lead_days = (len(look_slice)-1) - local_max_idx\n",
    "            peak_when = f\"t+{lead_days}d\"\n",
    "\n",
    "        # epi_year/week from the last actual date we have for this district in df_full_raw\n",
    "        gdist = df_full_raw[df_full_raw[\"district\"]==dist_name].sort_values(\"date\")\n",
    "        if len(gdist) > 0:\n",
    "            lastrow = gdist.iloc[-1]\n",
    "            last_date = pd.to_datetime(lastrow[\"date\"])\n",
    "            iso_year, iso_week, _ = last_date.isocalendar()\n",
    "            last_year = int(iso_year)\n",
    "            last_epi  = int(iso_week)\n",
    "            last_actual_week_cases = float(\n",
    "                gdist.tail(7)[\"daily_cases\"].sum()\n",
    "            )\n",
    "            this_week_actual = float(\n",
    "                gdist.tail(1)[\"daily_cases\"].sum()\n",
    "            )\n",
    "        else:\n",
    "            last_year = None\n",
    "            last_epi  = None\n",
    "            last_date = None\n",
    "            last_actual_week_cases = float(actual_prev)  # fallback\n",
    "            this_week_actual       = float(actual_curr)\n",
    "\n",
    "        # convert \"next day forecast\" to a pseudo \"next week forecast\"\n",
    "        # scale by 7 for dashboard-style weekly numbers\n",
    "        next_week_forecast = pred_next * 7.0\n",
    "        next_week_hi       = hi_next   * 7.0\n",
    "        next_week_lo       = lo_next   * 7.0\n",
    "        this_week_pred     = pred_curr * 7.0\n",
    "\n",
    "        # store for tables\n",
    "        district_names.append(dist_name)\n",
    "        last_actual_list.append(last_actual_week_cases)\n",
    "        current_actual_list.append(this_week_actual)\n",
    "        current_pred_list.append(this_week_pred)\n",
    "        next_day_pred_list.append(next_week_forecast)\n",
    "        next_day_hi_list.append(next_week_hi)\n",
    "        next_day_lo_list.append(next_week_lo)\n",
    "        growth_rate_list.append(growth)\n",
    "        conf_width_list.append(next_week_hi - next_week_lo)\n",
    "        peak_day_list.append(peak_when)\n",
    "        peak_value_list.append(peak_val * 7.0)\n",
    "        peak_hi_list.append(peak_hi * 7.0)\n",
    "        peak_leadtime_list.append(lead_days/7.0 if not isinstance(lead_days,float) or not math.isnan(lead_days) else np.nan)\n",
    "\n",
    "        last_epi_meta[dist_name] = {\n",
    "            \"Year\": last_year,\n",
    "            \"Epi_Week\": last_epi\n",
    "        }\n",
    "\n",
    "        epiweek_meta_rows.append({\n",
    "            \"District\": dist_name,\n",
    "            \"Year\": last_year,\n",
    "            \"Epi_Week\": last_epi,\n",
    "            \"Forecast_next_week_median\": float(next_week_forecast),\n",
    "            \"Forecast_next_week_hi\": float(next_week_hi),\n",
    "            \"Growth_flag\": classify_growth(growth)\n",
    "        })\n",
    "\n",
    "    # global calibration metrics\n",
    "    all_y_real = np.concatenate(all_y_real)\n",
    "    all_mu_real = np.concatenate(all_mu_real)\n",
    "    all_lo_real = np.concatenate(all_lo_real)\n",
    "    all_hi_real = np.concatenate(all_hi_real)\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"SMAPE\": smape(all_y_real, all_mu_real),\n",
    "        \"MSE\": mean_squared_error(all_y_real, all_mu_real),\n",
    "        \"RMSE\": math.sqrt(mean_squared_error(all_y_real, all_mu_real)),\n",
    "        \"R2\": r2_score(all_y_real, all_mu_real),\n",
    "        \"Coverage90\": float(np.mean((all_y_real >= all_lo_real) & (all_y_real <= all_hi_real))),\n",
    "    }\n",
    "\n",
    "    # data quality\n",
    "    quality_df_raw = calc_reporting_continuity_daily(df_full_raw, lookback_days=SEQ)\n",
    "    rep_col = [c for c in quality_df_raw.columns if \"Reporting_Continuity_Last\" in c][0]\n",
    "    quality_df = pd.merge(\n",
    "        pd.DataFrame({\"District\": district_names}),\n",
    "        quality_df_raw.rename(columns={\"district\":\"District\"}),\n",
    "        on=\"District\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    quality_df[\"Data_quality_flag\"] = quality_df[rep_col].apply(classify_data_quality)\n",
    "\n",
    "    # watchlist table\n",
    "    watchlist_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Expected_cases_next_week\": np.round(next_day_pred_list,2),\n",
    "        \"High_scenario_p90\":       np.round(next_day_hi_list,2),\n",
    "    })\n",
    "    watchlist_df[\"Status\"] = watchlist_df[\"High_scenario_p90\"].apply(classify_risk)\n",
    "    watchlist_df = watchlist_df.sort_values(\"High_scenario_p90\", ascending=False)\n",
    "\n",
    "    # overflow risk\n",
    "    capacity_lookup = {d: DEFAULT_CAPACITY_PER_DISTRICT for d in district_names}\n",
    "    overflow_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Capacity_threshold_beds_per_week\": [\n",
    "            capacity_lookup[d] for d in district_names\n",
    "        ],\n",
    "        \"Forecast_median_next_week\": np.round(next_day_pred_list,2),\n",
    "        \"High_scenario_p90\":        np.round(next_day_hi_list,2),\n",
    "    })\n",
    "    overflow_df[\"Breach_risk_flag\"] = [\n",
    "        \"YES\" if hi > capacity_lookup[d] else \"NO\"\n",
    "        for d, hi in zip(district_names, next_day_hi_list)\n",
    "    ]\n",
    "    overflow_df = overflow_df.sort_values(\"High_scenario_p90\", ascending=False)\n",
    "\n",
    "    # acceleration alerts\n",
    "    accel_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Last_week_cases\":        np.round(last_actual_list,2),\n",
    "        \"This_week_actual\":       np.round(current_actual_list,2),\n",
    "        \"This_week_predicted\":    np.round(current_pred_list,2),\n",
    "        \"Next_week_forecast\":     np.round(next_day_pred_list,2),\n",
    "        \"Growth_rate_WoW(%)\":     np.round(np.array(growth_rate_list)*100,1),\n",
    "    })\n",
    "    accel_df[\"Growth_flag\"] = accel_df[\"Growth_rate_WoW(%)\"].apply(lambda pct: classify_growth(pct/100.0))\n",
    "    accel_df = accel_df.sort_values(\"Growth_rate_WoW(%)\", ascending=False)\n",
    "\n",
    "    # forecast confidence\n",
    "    confidence_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Forecast_next_week\":            np.round(next_day_pred_list,2),\n",
    "        \"Uncertainty_width(p90-p10)\":    np.round(conf_width_list,2),\n",
    "    })\n",
    "    confidence_df[\"Confidence_flag\"] = confidence_df[\"Uncertainty_width(p90-p10)\"].apply(classify_conf)\n",
    "    confidence_df = confidence_df.sort_values(\"Uncertainty_width(p90-p10)\", ascending=True)\n",
    "\n",
    "    # peak projection\n",
    "    peak_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Peak_lead_time_weeks\": peak_leadtime_list,\n",
    "        \"Peak_cases_median\":    np.round(peak_value_list,2),\n",
    "        \"Peak_cases_high(p90)\": np.round(peak_hi_list,2),\n",
    "        \"Peak_when\":            peak_day_list\n",
    "    }).sort_values(\"Peak_cases_median\", ascending=False)\n",
    "\n",
    "    # isochrone-style spread table\n",
    "    isochrone_df = pd.DataFrame(epiweek_meta_rows)\n",
    "\n",
    "    # nowcast gap\n",
    "    gap_pct_list = []\n",
    "    for a, p in zip(current_actual_list, current_pred_list):\n",
    "        gap_pct = (p - a)/(a + 1e-6)*100.0\n",
    "        gap_pct_list.append(gap_pct)\n",
    "    nowcast_gap_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Current_week_actual\":               np.round(current_actual_list,2),\n",
    "        \"Current_week_predicted_from_prev\":  np.round(current_pred_list,2),\n",
    "        \"Nowcast_gap_percent\":               np.round(gap_pct_list,1),\n",
    "    }).sort_values(\"Nowcast_gap_percent\", ascending=False)\n",
    "\n",
    "    # ---------- Climate / lag influence with real variable names ----------\n",
    "    def parse_feature_name(raw_name: str):\n",
    "        \"\"\"\n",
    "        Examples:\n",
    "          rainfall_lag7      -> base_var='rainfall', lag_info='lag 7d'\n",
    "          temperature_rmean7 -> base_var='temperature', lag_info='7d rolling mean'\n",
    "          daily_cases_rstd14 -> base_var='daily_cases', lag_info='14d rolling std'\n",
    "          temperature        -> base_var='temperature', lag_info='same day'\n",
    "          doy_sin            -> base_var='season(doy)', lag_info=''\n",
    "          dow_cos            -> base_var='weekday(dow)', lag_info=''\n",
    "        \"\"\"\n",
    "        name = raw_name\n",
    "\n",
    "        # seasonal encodings\n",
    "        if name.startswith(\"doy_\"):\n",
    "            return \"season(doy)\", \"\"\n",
    "        if name.startswith(\"dow_\"):\n",
    "            return \"weekday(dow)\", \"\"\n",
    "\n",
    "        # rolling mean/std patterns\n",
    "        if \"_rmean\" in name:\n",
    "            base, tail = name.split(\"_rmean\")\n",
    "            return base, f\"{tail}d rolling mean\"\n",
    "        if \"_rstd\" in name:\n",
    "            base, tail = name.split(\"_rstd\")\n",
    "            return base, f\"{tail}d rolling std\"\n",
    "\n",
    "        # lag pattern\n",
    "        if \"_lag\" in name:\n",
    "            base, tail = name.split(\"_lag\")\n",
    "            return base, f\"lag {tail}d\"\n",
    "\n",
    "        # plain same-day vars\n",
    "        return name, \"same day\"\n",
    "\n",
    "    # use the feature vector from the \"current\" timestep (-2), excluding prev_y channel at end\n",
    "    last_feats_matrix = Xseq[:, -2, :-1]  # shape: [num_seq, num_features]\n",
    "    preds_arr = np.array(next_day_pred_list)\n",
    "\n",
    "    feat_import_rows = []\n",
    "    for fi in range(last_feats_matrix.shape[1]):\n",
    "        col_vals = last_feats_matrix[:, fi]\n",
    "\n",
    "        if np.std(col_vals) < 1e-8:\n",
    "            corr = 0.0\n",
    "        else:\n",
    "            corr = np.corrcoef(col_vals, preds_arr)[0, 1]\n",
    "\n",
    "        raw_feat_name = feat_cols[fi]\n",
    "        base_var, lag_info = parse_feature_name(raw_feat_name)\n",
    "\n",
    "        feat_import_rows.append({\n",
    "            \"feature\": raw_feat_name,\n",
    "            \"base_var\": base_var,\n",
    "            \"lag_info\": lag_info,\n",
    "            \"pearson_corr_with_next_week_forecast\": float(corr),\n",
    "        })\n",
    "\n",
    "    climate_influence_df = pd.DataFrame(feat_import_rows)\n",
    "    climate_influence_df[\"abs_corr\"] = (\n",
    "        climate_influence_df[\"pearson_corr_with_next_week_forecast\"].abs()\n",
    "    )\n",
    "    climate_influence_df = climate_influence_df.sort_values(\n",
    "        \"abs_corr\", ascending=False\n",
    "    )\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    # Executive summary\n",
    "    top5 = watchlist_df.head(5)[[\"District\",\"Expected_cases_next_week\",\"High_scenario_p90\",\"Status\"]].to_dict(orient=\"records\")\n",
    "    rapid = accel_df[accel_df[\"Growth_flag\"]==\"ðŸ”´ Rapid growth\"][\"District\"].tolist()\n",
    "    overflow_risk = overflow_df[overflow_df[\"Breach_risk_flag\"]==\"YES\"][\"District\"].tolist()\n",
    "    dq_bad = quality_df[quality_df[\"Data_quality_flag\"]==\"ðŸ”´ Needs field check\"][\"District\"].tolist()\n",
    "    summary_text = {\n",
    "        \"Top5_high_risk_next_week\": to_python(top5),\n",
    "        \"Districts_with_rapid_growth\": to_python(rapid),\n",
    "        \"Districts_with_capacity_breach_risk\": to_python(overflow_risk),\n",
    "        \"Districts_with_data_quality_issues\": to_python(dq_bad),\n",
    "        \"Model_calibration\": {\n",
    "            \"Coverage90\": round(float(overall_metrics[\"Coverage90\"]),3),\n",
    "            \"SMAPE\": round(float(overall_metrics[\"SMAPE\"]),2),\n",
    "            \"R2\": round(float(overall_metrics[\"R2\"]),3),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    products = {\n",
    "        \"watchlist_df\": watchlist_df,\n",
    "        \"overflow_df\": overflow_df,\n",
    "        \"accel_df\": accel_df,\n",
    "        \"confidence_df\": confidence_df,\n",
    "        \"quality_df\": quality_df.rename(columns={rep_col:\"reporting_continuity_pct\"}),\n",
    "        \"peak_df\": peak_df,\n",
    "        \"isochrone_df\": isochrone_df.rename(columns={\n",
    "            \"District\":\"District\",\n",
    "            \"Year\":\"Year\",\n",
    "            \"Epi_Week\":\"Epi_Week\",\n",
    "            \"Forecast_next_week_median\":\"Forecast_next_week_median\",\n",
    "            \"Forecast_next_week_hi\":\"Forecast_next_week_hi\",\n",
    "            \"Growth_flag\":\"Growth_flag\",\n",
    "        }),\n",
    "        \"nowcast_gap_df\": nowcast_gap_df,\n",
    "        \"climate_influence_df\": climate_influence_df,\n",
    "        \"executive_summary\": summary_text,\n",
    "        \"overall_metrics\": overall_metrics,\n",
    "        \"last_epi_meta\": last_epi_meta,\n",
    "    }\n",
    "\n",
    "    return overall_metrics, products\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# PUSH TO diarrhoea_* TABLES\n",
    "# =====================================================================================\n",
    "\n",
    "def push_products_to_db_diarrhoea(conn, products):\n",
    "    last_epi_meta = products[\"last_epi_meta\"]\n",
    "\n",
    "    # watchlist\n",
    "    watchlist_rows = []\n",
    "    for _, r in products[\"watchlist_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        watchlist_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Expected_cases_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Status\"]\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_watchlist\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"expected_cases_next_week\",\"high_scenario_p90\",\"status\"],\n",
    "        watchlist_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # overflow risk\n",
    "    overflow_rows = []\n",
    "    for _, r in products[\"overflow_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        overflow_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            int(r[\"Capacity_threshold_beds_per_week\"]) if not pd.isna(r[\"Capacity_threshold_beds_per_week\"]) else None,\n",
    "            py(r[\"Forecast_median_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Breach_risk_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_overflow_risk\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"capacity_threshold_beds_per_week\",\"forecast_median_next_week\",\"high_scenario_p90\",\"breach_risk_flag\"],\n",
    "        overflow_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # acceleration alerts\n",
    "    accel_rows = []\n",
    "    for _, r in products[\"accel_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        accel_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Last_week_cases\"]),\n",
    "            py(r[\"This_week_actual\"]),\n",
    "            py(r[\"This_week_predicted\"]),\n",
    "            py(r[\"Next_week_forecast\"]),\n",
    "            py(r[\"Growth_rate_WoW(%)\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_acceleration_alerts\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"last_week_cases\",\"this_week_actual\",\"this_week_predicted\",\"next_week_forecast\",\"growth_rate_wow\",\"growth_flag\"],\n",
    "        accel_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # confidence\n",
    "    conf_rows = []\n",
    "    for _, r in products[\"confidence_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        conf_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Forecast_next_week\"]),\n",
    "            py(r[\"Uncertainty_width(p90-p10)\"]),\n",
    "            r[\"Confidence_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_confidence\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"forecast_next_week\",\"uncertainty_width\",\"confidence_flag\"],\n",
    "        conf_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # surveillance quality\n",
    "    qual_rows = []\n",
    "    for _, r in products[\"quality_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        qual_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"reporting_continuity_pct\"]),\n",
    "            r[\"Data_quality_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_surveillance_quality\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"reporting_continuity_pct\",\"data_quality_flag\"],\n",
    "        qual_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # peak projection\n",
    "    peak_rows = []\n",
    "    for _, r in products[\"peak_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        peak_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Peak_lead_time_weeks\"]),\n",
    "            py(r[\"Peak_cases_median\"]),\n",
    "            py(r[\"Peak_cases_high(p90)\"]),\n",
    "            r[\"Peak_when\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_peak_projection\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"peak_lead_time_weeks\",\"peak_cases_median\",\"peak_cases_high_p90\",\"peak_when\"],\n",
    "        peak_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # isochrone-style spread\n",
    "    iso_rows = []\n",
    "    for _, r in products[\"isochrone_df\"].iterrows():\n",
    "        iso_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Year\"],\n",
    "            r[\"Epi_Week\"],\n",
    "            py(r[\"Forecast_next_week_median\"]),\n",
    "            py(r[\"Forecast_next_week_hi\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_isochrone_spread\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"forecast_next_week_median\",\"forecast_next_week_hi\",\"growth_flag\"],\n",
    "        iso_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # nowcast gap\n",
    "    gap_rows = []\n",
    "    for _, r in products[\"nowcast_gap_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        gap_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Current_week_actual\"]),\n",
    "            py(r[\"Current_week_predicted_from_prev\"]),\n",
    "            py(r[\"Nowcast_gap_percent\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_nowcast_gap\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"current_week_actual\",\"current_week_predicted_from_prev\",\"nowcast_gap_percent\"],\n",
    "        gap_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # climate influence\n",
    "    clim_rows = []\n",
    "    for _, r in products[\"climate_influence_df\"].iterrows():\n",
    "        clim_rows.append((\n",
    "            r[\"feature\"],\n",
    "            r[\"base_var\"],\n",
    "            r[\"lag_info\"],\n",
    "            py(r[\"pearson_corr_with_next_week_forecast\"]),\n",
    "            py(r[\"abs_corr\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_climate_influence\",\n",
    "        [\"feature\",\"base_var\",\"lag_info\",\"pearson_corr_with_next_week_forecast\",\"abs_corr\"],\n",
    "        clim_rows,\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "    # exec summary\n",
    "    summary_json = json.dumps(to_python(products[\"executive_summary\"]))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"diarrhoea_exec_summary\",\n",
    "        [\"summary\"],\n",
    "        [(summary_json,)],\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# MAIN PIPELINE\n",
    "# =====================================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"1) Load diarrhoea daily data from Postgres\")\n",
    "    df_daily = load_daily_from_db()\n",
    "\n",
    "    print(\"2) Add lags / rolling stats\")\n",
    "    df_daily, new_cols = add_lags_rolls_daily(df_daily)\n",
    "\n",
    "    # feature columns\n",
    "    feat_cols = [\n",
    "        \"temperature\",\"humidity\",\"rainfall\",\n",
    "        \"doy_sin\",\"doy_cos\",\"dow_sin\",\"dow_cos\"\n",
    "    ] + new_cols\n",
    "\n",
    "    target_col = \"daily_cases\"\n",
    "\n",
    "    # 3) Train/Val/Test split per district (daily horizon logic)\n",
    "    VAL_H = 14\n",
    "    TEST_H = 21\n",
    "    tr_parts, va_parts, te_parts = [], [], []\n",
    "    for dist, g in df_daily.groupby(\"district\"):\n",
    "        g = g.sort_values(\"date\")\n",
    "        if len(g) > (VAL_H + TEST_H):\n",
    "            te_parts.append(g.iloc[-TEST_H:])\n",
    "            va_parts.append(g.iloc[-(VAL_H + TEST_H):-TEST_H])\n",
    "            tr_parts.append(g.iloc[:-(VAL_H + TEST_H)])\n",
    "        elif len(g) > TEST_H:\n",
    "            te_parts.append(g.iloc[-TEST_H:])\n",
    "            tr_parts.append(g.iloc[:-TEST_H])\n",
    "        else:\n",
    "            tr_parts.append(g)\n",
    "    tr_df = pd.concat(tr_parts).reset_index(drop=True)\n",
    "    va_df = pd.concat(va_parts).reset_index(drop=True) if va_parts else tr_df.iloc[0:0].copy()\n",
    "    te_df = pd.concat(te_parts).reset_index(drop=True) if te_parts else tr_df.iloc[0:0].copy()\n",
    "\n",
    "    df_full_raw = df_daily.copy()\n",
    "\n",
    "    print(\"3) Fit label encoders / scalers on TRAIN\")\n",
    "    split_tr, le_tr, feat_scaler_tr, y_scalers_tr = build_mats(tr_df, feat_cols, target_col)\n",
    "\n",
    "    X_tr, Y_tr, C_tr, DoY_tr, DoW_tr = to_seq(split_tr, SEQ, prev_y=True)\n",
    "    if X_tr.shape[0] == 0:\n",
    "        raise RuntimeError(\"No train sequences. Reduce SEQ or check data continuity.\")\n",
    "    print(f\"Train sequences: {X_tr.shape}\")\n",
    "\n",
    "    # build val sequences\n",
    "    if len(va_df) > 0:\n",
    "        split_va = apply_mats(va_df, feat_cols, target_col, le_tr, feat_scaler_tr, y_scalers_tr)\n",
    "        X_va, Y_va, C_va, DoY_va, DoW_va = to_seq(split_va, SEQ, prev_y=True)\n",
    "        if X_va.shape[0] == 0:\n",
    "            X_va, Y_va, C_va, DoY_va, DoW_va = X_tr, Y_tr, C_tr, DoY_tr, DoW_tr\n",
    "    else:\n",
    "        X_va, Y_va, C_va, DoY_va, DoW_va = X_tr, Y_tr, C_tr, DoY_tr, DoW_tr\n",
    "\n",
    "    # build test sequences\n",
    "    if len(te_df) > 0:\n",
    "        split_te = apply_mats(te_df, feat_cols, target_col, le_tr, feat_scaler_tr, y_scalers_tr)\n",
    "        X_te, Y_te, C_te, DoY_te, DoW_te = to_seq(split_te, SEQ, prev_y=True)\n",
    "        if X_te.shape[0] == 0:\n",
    "            X_te, Y_te, C_te, DoY_te, DoW_te = X_va, Y_va, C_va, DoY_va, DoW_va\n",
    "    else:\n",
    "        X_te, Y_te, C_te, DoY_te, DoW_te = X_va, Y_va, C_va, DoY_va, DoW_va\n",
    "\n",
    "    nc = int(C_tr.max()) + 1\n",
    "    cond_dim = X_tr.shape[2]\n",
    "\n",
    "    print(\"4) Train diarrhoea model + early stop on val\")\n",
    "    M = train_model(\n",
    "        X_tr, Y_tr, C_tr, DoY_tr, DoW_tr,\n",
    "        X_va, Y_va, C_va, DoY_va, DoW_va,\n",
    "        cond_dim, nc\n",
    "    )\n",
    "    torch.save(M.state_dict(), os.path.join(OUT,\"diarrhoea_forecaster.pt\"))\n",
    "\n",
    "    print(\"5) Build director tables (using TEST split, like dengue dashboard style)\")\n",
    "    overall_metrics, products = evaluate_and_build_products(\n",
    "        M,\n",
    "        X_te, Y_te, C_te, DoY_te, DoW_te,\n",
    "        y_scalers_tr, le_tr, df_full_raw,\n",
    "        feat_cols\n",
    "    )\n",
    "\n",
    "    # save local artifacts for inspection\n",
    "    products[\"watchlist_df\"].to_csv(os.path.join(OUT,\"watchlist.csv\"), index=False)\n",
    "    products[\"overflow_df\"].to_csv(os.path.join(OUT,\"overflow_risk.csv\"), index=False)\n",
    "    products[\"accel_df\"].to_csv(os.path.join(OUT,\"acceleration_alerts.csv\"), index=False)\n",
    "    products[\"confidence_df\"].to_csv(os.path.join(OUT,\"forecast_confidence.csv\"), index=False)\n",
    "    products[\"quality_df\"].to_csv(os.path.join(OUT,\"surveillance_quality.csv\"), index=False)\n",
    "    products[\"peak_df\"].to_csv(os.path.join(OUT,\"peak_projection.csv\"), index=False)\n",
    "    products[\"isochrone_df\"].to_csv(os.path.join(OUT,\"isochrone_spread.csv\"), index=False)\n",
    "    products[\"nowcast_gap_df\"].to_csv(os.path.join(OUT,\"nowcast_gap.csv\"), index=False)\n",
    "    products[\"climate_influence_df\"].to_csv(os.path.join(OUT,\"climate_lag_influence.csv\"), index=False)\n",
    "\n",
    "    dash_manifest = {\n",
    "        \"executive_summary\": products[\"executive_summary\"],\n",
    "        \"overall_metrics\": products[\"overall_metrics\"],\n",
    "        \"notes\": \"Auto-generated diarrhoea decision tables (daily model projected to weekly scale).\"\n",
    "    }\n",
    "    with open(os.path.join(OUT,\"DASHBOARD_SUMMARY.json\"), \"w\") as f:\n",
    "        json.dump(to_python(dash_manifest), f, indent=2)\n",
    "\n",
    "    print(\"6) Upsert diarrhoea_* tables in Postgres\")\n",
    "    conn = get_db_conn()\n",
    "    ensure_output_tables_diarrhoea(conn)\n",
    "    push_products_to_db_diarrhoea(conn, products)\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n=== Executive Summary Preview ===\")\n",
    "    print(json.dumps(to_python(dash_manifest[\"executive_summary\"]), indent=2))\n",
    "    print(\"\\nArtifacts saved to\", OUT, \"and pushed to Postgres.\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
