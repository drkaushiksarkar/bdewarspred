{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58134d53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# ----------------- Helpers for serialization / DB safety -----------------\n",
    "\n",
    "def to_python(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert numpy / pandas scalars, arrays, and NaN into\n",
    "    vanilla Python objects so they're JSON serializable.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        val = float(obj)\n",
    "        if math.isnan(val):\n",
    "            return None\n",
    "        return val\n",
    "    if isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return [to_python(x) for x in obj.tolist()]\n",
    "\n",
    "    if isinstance(obj, (pd.Timestamp,)):\n",
    "        if pd.isna(obj):\n",
    "            return None\n",
    "        return obj.isoformat()\n",
    "\n",
    "    if isinstance(obj, float):\n",
    "        if math.isnan(obj):\n",
    "            return None\n",
    "        return obj\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_python(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [to_python(v) for v in obj]\n",
    "\n",
    "    return obj\n",
    "\n",
    "def py(v):\n",
    "    \"\"\"\n",
    "    Cast values for Postgres. Keep table schemas UNCHANGED.\n",
    "    \"\"\"\n",
    "    if isinstance(v, (np.floating, np.float32, np.float64)):\n",
    "        v = float(v)\n",
    "    elif isinstance(v, (np.integer, np.int32, np.int64)):\n",
    "        v = int(v)\n",
    "    elif isinstance(v, (np.bool_,)):\n",
    "        v = bool(v)\n",
    "\n",
    "    if isinstance(v, float) and (math.isnan(v)):\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "\n",
    "PG_HOST    = \"119.148.17.102\"\n",
    "PG_PORT    = 5432\n",
    "PG_DB      = \"ewarsdb\"\n",
    "PG_USER    = \"ewars\"\n",
    "PG_PASS    = \"Iedcr@Ewars2025\"\n",
    "PG_SSLMODE = \"require\"\n",
    "\n",
    "OUT = \"/content/weekly_dengue_out\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEQ = 16\n",
    "BATCH = 64\n",
    "EPOCHS_GAN = 400\n",
    "PATIENCE = 25\n",
    "NOISE = 12\n",
    "LSTM_UNITS = 128\n",
    "HEADS = 8\n",
    "DROP = 0.25\n",
    "LR_G = 8e-4\n",
    "LR_D = 3e-4\n",
    "WD = 1e-4\n",
    "CLIP = 1.0\n",
    "TTUR = (1.0, 1.0)\n",
    "TF_START = 1.0\n",
    "TF_END = 0.3\n",
    "ALPHA = 0.5\n",
    "K_SYNC = 5\n",
    "\n",
    "Q = (0.1, 0.5, 0.9)\n",
    "ABL = {\"adv\": True, \"hetero\": True, \"quant\": True}\n",
    "\n",
    "NSIG = (0.1, 1.2)\n",
    "K_MC = 50\n",
    "VAL_H_WEEKS = 6\n",
    "TEST_H_WEEKS = 6\n",
    "\n",
    "DEFAULT_CAPACITY_PER_DISTRICT = 25\n",
    "RAPID_GROWTH_THRESH = 0.30\n",
    "MODERATE_GROWTH_THRESH = 0.10\n",
    "PEAK_LOOKAHEAD_WEEKS = 6\n",
    "\n",
    "# widen fallback Gaussian bands in case we still call copula_bands\n",
    "SIG_CALIBRATION_FACTOR = 1.5\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# ----------------- DB helpers (unchanged schemas) -----------------\n",
    "\n",
    "def get_db_conn():\n",
    "    dsn = (\n",
    "        f\"host={PG_HOST} port={PG_PORT} dbname={PG_DB} \"\n",
    "        f\"user={PG_USER} password={PG_PASS} sslmode={PG_SSLMODE}\"\n",
    "    )\n",
    "    return psycopg2.connect(dsn)\n",
    "\n",
    "def ensure_output_tables(conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_watchlist (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        expected_cases_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        status TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_overflow_risk (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        capacity_threshold_beds_per_week INT,\n",
    "        forecast_median_next_week NUMERIC,\n",
    "        high_scenario_p90 NUMERIC,\n",
    "        breach_risk_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_acceleration_alerts (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        last_week_cases NUMERIC,\n",
    "        this_week_actual NUMERIC,\n",
    "        this_week_predicted NUMERIC,\n",
    "        next_week_forecast NUMERIC,\n",
    "        growth_rate_wow NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_confidence (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        forecast_next_week NUMERIC,\n",
    "        uncertainty_width NUMERIC,\n",
    "        confidence_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_surveillance_quality (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        reporting_continuity_pct NUMERIC,\n",
    "        data_quality_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_peak_projection (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        peak_lead_time_weeks NUMERIC,\n",
    "        peak_cases_median NUMERIC,\n",
    "        peak_cases_high_p90 NUMERIC,\n",
    "        peak_when TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_isochrone_spread (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        forecast_next_week_median NUMERIC,\n",
    "        forecast_next_week_hi NUMERIC,\n",
    "        growth_flag TEXT,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_nowcast_gap (\n",
    "        district TEXT NOT NULL,\n",
    "        year INT,\n",
    "        epi_week INT,\n",
    "        current_week_actual NUMERIC,\n",
    "        current_week_predicted_from_prev NUMERIC,\n",
    "        nowcast_gap_percent NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now(),\n",
    "        PRIMARY KEY (district, year, epi_week)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_climate_influence (\n",
    "        feature TEXT,\n",
    "        base_var TEXT,\n",
    "        lag_info TEXT,\n",
    "        pearson_corr_with_next_week_forecast NUMERIC,\n",
    "        abs_corr NUMERIC,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dengue_exec_summary (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        summary JSONB,\n",
    "        created_at TIMESTAMP DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "def _dedupe_by_pk(rows, cols, pk_cols):\n",
    "    if not pk_cols:\n",
    "        return rows\n",
    "    pk_idx = [cols.index(pk) for pk in pk_cols]\n",
    "    dedup_map = {}\n",
    "    for row in rows:\n",
    "        key = tuple(row[i] for i in pk_idx)\n",
    "        dedup_map[key] = row\n",
    "    return list(dedup_map.values())\n",
    "\n",
    "def upsert_table(conn, table_name, cols, rows, pk_cols=None, wipe_first=False):\n",
    "    if wipe_first:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"TRUNCATE TABLE {table_name};\")\n",
    "        conn.commit()\n",
    "\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    if not pk_cols:\n",
    "        with conn.cursor() as cur:\n",
    "            insert_sql = (\n",
    "                f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s\"\n",
    "            )\n",
    "            execute_values(cur, insert_sql, rows)\n",
    "        conn.commit()\n",
    "        return\n",
    "\n",
    "    rows_dedup = _dedupe_by_pk(rows, cols, pk_cols)\n",
    "    conflict_target = \", \".join(pk_cols)\n",
    "    set_updates = \", \".join([\n",
    "        f\"{c}=EXCLUDED.{c}\" for c in cols if c not in pk_cols\n",
    "    ])\n",
    "    insert_sql = (\n",
    "        f\"INSERT INTO {table_name} ({', '.join(cols)}) VALUES %s \"\n",
    "        f\"ON CONFLICT ({conflict_target}) DO UPDATE SET {set_updates};\"\n",
    "    )\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        execute_values(cur, insert_sql, rows_dedup)\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# ----------------- Feature engineering / model code -----------------\n",
    "\n",
    "def _iso_week_start(year, week):\n",
    "    iso_year = str(int(year))\n",
    "    iso_week = str(int(week)).zfill(2)\n",
    "    return pd.to_datetime(iso_year + iso_week + \"1\", format=\"%G%V%u\", errors=\"coerce\")\n",
    "\n",
    "def fetch_dengue_weather_from_db():\n",
    "    conn = get_db_conn()\n",
    "    try:\n",
    "        try_query = \"\"\"\n",
    "            SELECT\n",
    "                \"District\",\n",
    "                \"Division\",\n",
    "                \"Year\",\n",
    "                \"Epi_Week\",\n",
    "                weekly_hospitalised_cases,\n",
    "                \"Total_Rainfall\",\n",
    "                \"Avg_Humidity\",\n",
    "                \"Avg_Temperature\"\n",
    "            FROM vw_dengue_weekly_input\n",
    "            ORDER BY \"District\",\"Year\",\"Epi_Week\";\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_sql(try_query, conn)\n",
    "            return df\n",
    "        except Exception:\n",
    "            fallback_query = \"\"\"\n",
    "                SELECT\n",
    "                    district      AS \"District\",\n",
    "                    division      AS \"Division\",\n",
    "                    year          AS \"Year\",\n",
    "                    epi_week      AS \"Epi_Week\",\n",
    "                    weekly_hospitalised_cases,\n",
    "                    total_rainfall    AS \"Total_Rainfall\",\n",
    "                    avg_humidity      AS \"Avg_Humidity\",\n",
    "                    avg_temperature   AS \"Avg_Temperature\"\n",
    "                FROM dengue_weather\n",
    "                WHERE year IS NOT NULL\n",
    "                  AND epi_week IS NOT NULL\n",
    "                ORDER BY district, year, epi_week;\n",
    "            \"\"\"\n",
    "            df = pd.read_sql(fallback_query, conn)\n",
    "            return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def load_weekly_from_db():\n",
    "    df = fetch_dengue_weather_from_db()\n",
    "\n",
    "    df[\"District\"] = df[\"District\"].astype(str).str.strip()\n",
    "    df[\"Division\"] = df[\"Division\"].astype(str).str.strip()\n",
    "    df[\"Year\"]     = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"Epi_Week\"] = pd.to_numeric(df[\"Epi_Week\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"weekly_hospitalised_cases\",\n",
    "        \"Total_Rainfall\",\n",
    "        \"Avg_Humidity\",\n",
    "        \"Avg_Temperature\",\n",
    "    ]\n",
    "    optional_numeric = [\"Avg_NDVI\", \"Avg_NDWI\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df[\"week_start\"] = [\n",
    "        _iso_week_start(y, w)\n",
    "        for (y, w) in zip(df[\"Year\"], df[\"Epi_Week\"])\n",
    "    ]\n",
    "    mask = df[\"week_start\"].isna()\n",
    "    if mask.any():\n",
    "        tmp = pd.to_datetime(df.loc[mask, \"Year\"].astype(str) + \"-01-01\", errors=\"coerce\")\n",
    "        df.loc[mask, \"week_start\"] = tmp + pd.to_timedelta(\n",
    "            (df.loc[mask, \"Epi_Week\"].astype(float) - 1) * 7,\n",
    "            unit=\"D\"\n",
    "        )\n",
    "\n",
    "    df[\"Month\"] = df[\"week_start\"].dt.month.astype(\"Int64\")\n",
    "    df[\"WeekOfYear\"] = df[\"week_start\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "    df = df.sort_values([\"District\", \"Year\", \"Epi_Week\"]).reset_index(drop=True)\n",
    "\n",
    "    fill_cols = numeric_cols + [c for c in optional_numeric if c in df.columns]\n",
    "    df[fill_cols] = df[fill_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    df[fill_cols] = (\n",
    "        df.groupby(\"District\")[fill_cols]\n",
    "          .apply(lambda g: g.ffill().bfill())\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df[fill_cols] = df[fill_cols].fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_time_features(df):\n",
    "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"WeekOfYear\"].astype(float) / 52.0).astype(np.float32)\n",
    "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"WeekOfYear\"].astype(float) / 52.0).astype(np.float32)\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"Month\"].astype(float) / 12.0).astype(np.float32)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"Month\"].astype(float) / 12.0).astype(np.float32)\n",
    "    df[\"Year_num\"] = df[\"Year\"].astype(float)\n",
    "    return df\n",
    "\n",
    "def add_incidence_derivs(df):\n",
    "    \"\"\"\n",
    "    Surge velocity features:\n",
    "    - week-over-week growth\n",
    "    - 2-week growth\n",
    "    - acceleration of growth\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"District\",\"Year\",\"Epi_Week\"]).copy()\n",
    "    grp = df.groupby(\"District\")[\"weekly_hospitalised_cases\"]\n",
    "\n",
    "    prev1 = grp.shift(1)\n",
    "    prev2 = grp.shift(2)\n",
    "\n",
    "    df[\"case_growth_1w\"] = ((grp.shift(0) - prev1) / (prev1 + 1e-6)).fillna(0.0)\n",
    "    df[\"case_growth_2w\"] = ((grp.shift(0) - prev2) / (prev2 + 1e-6)).fillna(0.0)\n",
    "    df[\"acceleration\"]   = (df[\"case_growth_1w\"] - df[\"case_growth_2w\"]).fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_static_district_feats(df):\n",
    "    \"\"\"\n",
    "    District static context (beds/pop/etc). For now we just give each district\n",
    "    DEFAULT_CAPACITY_PER_DISTRICT so model can learn \"scale\".\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cap_map = {}\n",
    "    for dist in df[\"District\"].unique():\n",
    "        cap_map[dist] = DEFAULT_CAPACITY_PER_DISTRICT\n",
    "    df[\"district_capacity_proxy\"] = df[\"District\"].map(cap_map).astype(float)\n",
    "    return df\n",
    "\n",
    "def add_lags_rolls(df, base_cols, lags=(1, 3, 6, 12), rolls=(3, 6, 12)):\n",
    "    df = df.sort_values([\"District\", \"Year\", \"Epi_Week\"]).copy()\n",
    "    new_cols = []\n",
    "    for col in base_cols:\n",
    "        for lag in lags:\n",
    "            lag_col = f\"{col}_lag{lag}\"\n",
    "            df[lag_col] = df.groupby(\"District\")[col].shift(lag)\n",
    "            new_cols.append(lag_col)\n",
    "        for window in rolls:\n",
    "            mean_col = f\"{col}_rmean{window}\"\n",
    "            std_col = f\"{col}_rstd{window}\"\n",
    "            grp = df.groupby(\"District\")[col]\n",
    "            df[mean_col] = grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            df[std_col]  = grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True).fillna(0.0)\n",
    "            new_cols.extend([mean_col, std_col])\n",
    "    df[new_cols] = df[new_cols].fillna(0.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------- dataset prep -----------------\n",
    "\n",
    "class Split:\n",
    "    def __init__(self, X, y, c, df):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.c = c\n",
    "        self.df = df\n",
    "\n",
    "def build_mats(df, feat, target):\n",
    "    le = LabelEncoder().fit(df[\"District\"].values)\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"district_id\"] = le.transform(d[\"District\"])\n",
    "\n",
    "    X = d[feat].values.astype(np.float32)\n",
    "    feat_scaler = StandardScaler().fit(X)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y = np.log1p(np.clip(d[target].values.reshape(-1, 1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "    y_scalers = {}\n",
    "    for cid, g in d.groupby(\"district_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = StandardScaler().fit(y[idx])\n",
    "        y_scalers[cid] = sc\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"district_id\"].values.astype(np.int64), d), le, feat_scaler, y_scalers\n",
    "\n",
    "def apply_mats(df, feat, target, le, feat_scaler, y_scalers):\n",
    "    d = df.copy()\n",
    "    d[\"district_id\"] = le.transform(d[\"District\"])\n",
    "\n",
    "    X = d[feat].values.astype(np.float32)\n",
    "    Xs = feat_scaler.transform(X)\n",
    "\n",
    "    y = np.log1p(np.clip(d[target].values.reshape(-1, 1), 0, None)).astype(np.float32)\n",
    "    Ys = np.zeros_like(y, np.float32)\n",
    "\n",
    "    for cid, g in d.groupby(\"district_id\"):\n",
    "        idx = g.index.values\n",
    "        sc = y_scalers.get(int(cid), StandardScaler().fit(y[idx]))\n",
    "        Ys[idx] = sc.transform(y[idx])\n",
    "\n",
    "    return Split(Xs, Ys, d[\"district_id\"].values.astype(np.int64), d)\n",
    "\n",
    "def to_seq(split, L=SEQ, prev_y=True):\n",
    "    X, y, c, df = split.X, split.y, split.c, split.df\n",
    "    iso_idx = (df[\"Year\"].astype(int) * 100 + df[\"Epi_Week\"].astype(int)).values\n",
    "    SX, SY, SC = [], [], []\n",
    "    for cid in np.unique(c):\n",
    "        idx = np.where(c == cid)[0]\n",
    "        order = np.argsort(iso_idx[idx])\n",
    "        idx = idx[order]\n",
    "        for i in range(len(idx) - L + 1):\n",
    "            sl = idx[i : i + L]\n",
    "            Xi = X[sl]\n",
    "            Yi = y[sl]\n",
    "            if prev_y:\n",
    "                prev = np.vstack([np.zeros((1, 1), np.float32), Yi[:-1]])\n",
    "                Xi = np.concatenate([Xi, prev], 1)\n",
    "            SX.append(Xi)\n",
    "            SY.append(Yi)\n",
    "            SC.append(cid)\n",
    "    return np.asarray(SX, np.float32), np.asarray(SY, np.float32), np.asarray(SC, np.int64)\n",
    "\n",
    "\n",
    "# ----------------- model -----------------\n",
    "\n",
    "class CausalTCN(nn.Module):\n",
    "    def __init__(self, in_ch, hid=64, levels=3, k=3):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        ch = in_ch\n",
    "        for level in range(levels):\n",
    "            dil = 2 ** level\n",
    "            pad = (k - 1) * dil\n",
    "            self.blocks.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(ch, hid, kernel_size=k, dilation=dil, padding=pad),\n",
    "                    nn.GELU()\n",
    "                )\n",
    "            )\n",
    "            ch = hid\n",
    "    def forward(self, x):\n",
    "        y = x.transpose(1, 2)\n",
    "        L = y.size(-1)\n",
    "        for block in self.blocks:\n",
    "            y = block(y)\n",
    "            # causal crop\n",
    "            y = y[..., :L]\n",
    "        return y.transpose(1, 2)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, cond_dim, noise_dim, nc, emb=8, lstm=LSTM_UNITS, heads=HEADS, drop=DROP, qu=Q):\n",
    "        super().__init__()\n",
    "        self.q = qu\n",
    "        self.ce = nn.Embedding(nc, emb)\n",
    "        self.pn = nn.Linear(noise_dim, cond_dim)\n",
    "        self.tcn = CausalTCN(cond_dim * 2 + emb, hid=lstm)\n",
    "        self.lstm = nn.LSTM(lstm, lstm, 1, batch_first=True)\n",
    "        self.mha = nn.MultiheadAttention(lstm, heads, batch_first=True, dropout=drop)\n",
    "        self.ln = nn.LayerNorm(lstm)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.mu = nn.Linear(lstm, 1)\n",
    "        self.ls = nn.Linear(lstm, 1)\n",
    "        self.qh = nn.ModuleList([nn.Linear(lstm, 1) for _ in qu])\n",
    "        self.last = None\n",
    "    def forward(self, cond, noise, cid):\n",
    "        B, L, _ = cond.shape\n",
    "        emb = self.ce(cid).unsqueeze(1).repeat(1, L, 1)\n",
    "        z = self.pn(noise)\n",
    "        h = self.tcn(torch.cat([cond, z, emb], -1))\n",
    "        h, _ = self.lstm(h)\n",
    "        mask = torch.triu(torch.ones(L, L, device=h.device, dtype=torch.bool), diagonal=1)\n",
    "        att, weights = self.mha(h, h, h, attn_mask=mask, need_weights=True)\n",
    "        self.last = weights.detach()\n",
    "        h = self.drop(self.ln(att))\n",
    "        mu = self.mu(h)\n",
    "        ls = torch.clamp(self.ls(h), -5.0, 3.0)\n",
    "        qs = [head(h) for head in self.qh]  # q10, q50, q90\n",
    "        return mu, ls, qs\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, cond_dim, nc, emb=8, lstm=LSTM_UNITS, drop=DROP):\n",
    "        super().__init__()\n",
    "        self.ce = nn.Embedding(nc, emb)\n",
    "        self.lstm = nn.LSTM(cond_dim + 1 + emb, lstm, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm, 64)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    def forward(self, cond, ts, cid):\n",
    "        B, L, _ = cond.shape\n",
    "        emb = self.ce(cid).unsqueeze(1).repeat(1, L, 1)\n",
    "        h, _ = self.lstm(torch.cat([cond, ts, emb], -1))\n",
    "        feat = F.gelu(self.fc(h[:, -1, :]))\n",
    "        return self.out(self.drop(feat)).squeeze(1), feat\n",
    "\n",
    "\n",
    "# ----------------- loss / train utils -----------------\n",
    "\n",
    "def smape(y, p):\n",
    "    y = y.flatten(); p = p.flatten()\n",
    "    return 100 * np.mean(2 * np.abs(p - y) / (np.abs(y) + np.abs(p) + 1e-8))\n",
    "\n",
    "def lerp(a, b, t): return {k: a[k] + (b[k] - a[k]) * t for k in a}\n",
    "\n",
    "def pinball(pred, target, quantile):\n",
    "    err = target - pred\n",
    "    return torch.mean(torch.maximum(quantile * err, (quantile - 1) * err))\n",
    "\n",
    "def tv_l1(x):\n",
    "    diff = x[:, 1:] - x[:, :-1]\n",
    "    return diff.abs().mean(), x.abs().mean()\n",
    "\n",
    "def gp(critic, yr, yf, cond, cc, lam=10.0):\n",
    "    B = yr.size(0)\n",
    "    eps = torch.rand(B, 1, 1, device=DEVICE)\n",
    "    xi = (eps * yr + (1 - eps) * yf).requires_grad_(True)\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        score, _ = critic(cond, xi, cc)\n",
    "    grad = torch.autograd.grad(\n",
    "        score, xi,\n",
    "        grad_outputs=torch.ones_like(score),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    return ((grad.view(B, -1).norm(2, 1) - 1) ** 2).mean() * lam\n",
    "\n",
    "def outbreak_penalty(mu, Y):\n",
    "    \"\"\"\n",
    "    Extra weight when model underpredicts outbreaks.\n",
    "    \"\"\"\n",
    "    diff_pos = (Y - mu).clamp(min=0.0)\n",
    "    return diff_pos.mean()\n",
    "\n",
    "def _g_loss(G, D, X, Y, C, noise, weights, pc_idx):\n",
    "    mu, ls, qs = G(X, noise, C)\n",
    "    sig = (ls.exp()).clamp(1e-3, 50.0)\n",
    "\n",
    "    if ABL[\"hetero\"]:\n",
    "        nll = 0.5 * (((Y - mu) / sig) ** 2 + 2 * ls + math.log(2 * math.pi)).mean()\n",
    "    else:\n",
    "        nll = F.l1_loss(mu, Y)\n",
    "\n",
    "    q_loss = torch.tensor(0.0, device=DEVICE)\n",
    "    if ABL[\"quant\"]:\n",
    "        for i, qv in enumerate(Q):\n",
    "            q_loss += pinball(qs[i], Y, qv)\n",
    "        q_loss /= len(Q)\n",
    "\n",
    "    if len(pc_idx) > 0:\n",
    "        tv, l1 = tv_l1(X[..., pc_idx])\n",
    "    else:\n",
    "        tv = torch.tensor(0.0, device=DEVICE)\n",
    "        l1 = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "    fm = torch.tensor(0.0, device=DEVICE)\n",
    "    adv_term = torch.tensor(0.0, device=DEVICE)\n",
    "    if ABL[\"adv\"]:\n",
    "        with torch.no_grad():\n",
    "            _, real_feat = D(X, Y, C)\n",
    "        score_fake, fake_feat = D(X, mu, C)\n",
    "        fm = F.l1_loss(fake_feat, real_feat)\n",
    "        adv_term = -score_fake.mean()\n",
    "\n",
    "    pen = outbreak_penalty(mu, Y)  # outbreak underprediction cost\n",
    "    outbreak_w = 0.2\n",
    "\n",
    "    return (\n",
    "        weights[\"nll\"]*nll\n",
    "        + weights[\"q\"]*q_loss\n",
    "        + weights[\"tv\"]*tv\n",
    "        + weights[\"l1\"]*l1\n",
    "        + (weights[\"fm\"]*fm if ABL[\"adv\"] else 0.0)\n",
    "        + (weights[\"adv\"]*adv_term if ABL[\"adv\"] else 0.0)\n",
    "        + outbreak_w * pen\n",
    "    )\n",
    "\n",
    "def validate(G, Xv, Yv, Cv):\n",
    "    \"\"\"\n",
    "    Validation: SMAPE on scaled space + coverage using direct quantiles.\n",
    "    \"\"\"\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(Xv, dtype=torch.float32, device=DEVICE)\n",
    "        C = torch.tensor(Cv, dtype=torch.long, device=DEVICE)\n",
    "        B,L,_ = X.shape\n",
    "\n",
    "        mu, ls, qs = G(X, torch.zeros(B,L,NOISE,device=DEVICE), C)\n",
    "\n",
    "        mp = mu[...,0].cpu().numpy()\n",
    "        y  = Yv[...,0]\n",
    "        q10 = qs[0][...,0].cpu().numpy()\n",
    "        q90 = qs[2][...,0].cpu().numpy()\n",
    "\n",
    "        coverage = float(np.mean((y>=q10)&(y<=q90)))\n",
    "        score    = smape(y.reshape(-1), mp.reshape(-1))\n",
    "    return score, coverage\n",
    "\n",
    "\n",
    "# ----------------- optimizer wrappers -----------------\n",
    "\n",
    "class Lookahead(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Lightweight lookahead wrapper around a base optimizer.\n",
    "    We'll just hold a slow copy of every param and sync every k steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_optimizer, alpha=ALPHA, k=K_SYNC):\n",
    "        self.base = base_optimizer\n",
    "        self.param_groups = self.base.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self._step = 0\n",
    "\n",
    "        # keep slow weights\n",
    "        self.slow_weights = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    self.slow_weights.append(p.detach().clone())\n",
    "                else:\n",
    "                    self.slow_weights.append(None)\n",
    "\n",
    "    def zero_grad(self, set_to_none=True):\n",
    "        self.base.zero_grad(set_to_none=set_to_none)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.base.step(closure)\n",
    "        self._step += 1\n",
    "        if self._step % self.k != 0:\n",
    "            return loss\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    slow = self.slow_weights[idx]\n",
    "                    slow.add_(self.alpha, p.data - slow)\n",
    "                    p.data.copy_(slow)\n",
    "                    self.slow_weights[idx] = slow.clone()\n",
    "                idx += 1\n",
    "        return loss\n",
    "\n",
    "class SAM:\n",
    "    \"\"\"\n",
    "    Sharpness-Aware Minimization helper around an optimizer.\n",
    "    Usage:\n",
    "        loss.backward()\n",
    "        sam.first_step()\n",
    "        second_loss.backward()\n",
    "        sam.second_step()\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, rho=0.05, adaptive=True):\n",
    "        self.optimizer = optimizer      # should be Lookahead-wrapped AdamW\n",
    "        self.rho = rho\n",
    "        self.adaptive = adaptive\n",
    "        self.e_ws = {}                  # store perturbations per param\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _grad_norm(self):\n",
    "        norms=[]\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if self.adaptive:\n",
    "                    norms.append(((p.abs()) * p.grad).norm(p=2))\n",
    "                else:\n",
    "                    norms.append((p.grad).norm(p=2))\n",
    "        if not norms:\n",
    "            return torch.tensor(0.0, device=DEVICE)\n",
    "        return torch.norm(torch.stack(norms), p=2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # compute scaled e_w and add it to params\n",
    "        grad_norm = self._grad_norm()\n",
    "        scale = self.rho / (grad_norm + 1e-12)\n",
    "        self.e_ws = {}\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if self.adaptive:\n",
    "                    e_w = (p.abs() * p.grad) * scale\n",
    "                else:\n",
    "                    e_w = p.grad * scale\n",
    "                p.add_(e_w)\n",
    "                self.e_ws[p] = e_w\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, clip_norm=CLIP):\n",
    "        # restore weights\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p in self.e_ws:\n",
    "                    p.sub_(self.e_ws[p])\n",
    "        # clip grads then take optimizer step\n",
    "        params = []\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is not None:\n",
    "                    params.append(p)\n",
    "        if len(params) > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(params, clip_norm)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        self.e_ws = {}\n",
    "\n",
    "def train_gan(Xtr,Ytr,Ctr,Xva,Yva,Cva,cond_dim,nc,pc_idx):\n",
    "    G=Generator(cond_dim=cond_dim,noise_dim=NOISE,nc=nc).to(DEVICE)\n",
    "    D=Critic(cond_dim=cond_dim,nc=nc).to(DEVICE)\n",
    "\n",
    "    baseG = torch.optim.AdamW(G.parameters(),lr=LR_G*TTUR[0],betas=(0.9,0.999),weight_decay=WD)\n",
    "    baseD = torch.optim.AdamW(D.parameters(),lr=LR_D*TTUR[1],betas=(0.9,0.999),weight_decay=WD)\n",
    "    optG  = Lookahead(baseG, alpha=ALPHA, k=K_SYNC)\n",
    "    optD  = Lookahead(baseD, alpha=ALPHA, k=K_SYNC)\n",
    "    sam   = SAM(optG, rho=0.05, adaptive=True)\n",
    "\n",
    "    dl=DataLoader(SeqDS(Xtr,Ytr,Ctr),batch_size=BATCH,shuffle=True,drop_last=True)\n",
    "\n",
    "    best=float(\"inf\"); best_sd=None; wait=0\n",
    "    for epoch in range(EPOCHS_GAN):\n",
    "        t=epoch/(EPOCHS_GAN-1)\n",
    "        noise_scale = NSIG[0]+(NSIG[1]-NSIG[0])*t\n",
    "        W_START={\"nll\":1.0,\"q\":0.5,\"tv\":0.05,\"l1\":0.02,\"fm\":0.2,\"adv\":0.5}\n",
    "        W_END  ={\"nll\":1.0,\"q\":1.0,\"tv\":0.10,\"l1\":0.05,\"fm\":0.1,\"adv\":0.4}\n",
    "        weights=lerp(W_START,W_END,t)\n",
    "        tf=TF_START+(TF_END-TF_START)*t\n",
    "\n",
    "        G.train(); D.train()\n",
    "        for Xb,Yb,Cb in dl:\n",
    "            Xb=torch.tensor(Xb,dtype=torch.float32,device=DEVICE)\n",
    "            Yb=torch.tensor(Yb,dtype=torch.float32,device=DEVICE)\n",
    "            Cb=torch.tensor(Cb,dtype=torch.long,device=DEVICE)\n",
    "            B,L,_=Xb.shape\n",
    "\n",
    "            # teacher forcing schedule (bleeding in model prev_y)\n",
    "            with torch.no_grad():\n",
    "                mu0,_,_=G(Xb,torch.zeros(B,L,NOISE,device=DEVICE),Cb)\n",
    "                prev=torch.cat([torch.zeros(B,1,device=DEVICE),mu0[:,:-1,0]],1)\n",
    "            Xb[:,:,-1]=tf*Xb[:,:,-1]+(1-tf)*prev\n",
    "\n",
    "            # --- train discriminator ---\n",
    "            if ABL[\"adv\"]:\n",
    "                optD.zero_grad(set_to_none=True)\n",
    "                z=torch.randn(B,L,NOISE,device=DEVICE)*noise_scale\n",
    "                mu,_,_=G(Xb,z,Cb)\n",
    "                Y_fake=mu.detach()\n",
    "                score_real,_=D(Xb,Yb,Cb)\n",
    "                score_fake,_=D(Xb,Y_fake,Cb)\n",
    "                d_loss=-(score_real.mean()-score_fake.mean())+gp(D,Yb,Y_fake,Xb,Cb,10.0)\n",
    "                d_loss.backward()\n",
    "                # clip & step discriminator\n",
    "                d_params = []\n",
    "                for group in optD.param_groups:\n",
    "                    for p in group[\"params\"]:\n",
    "                        if p.grad is not None:\n",
    "                            d_params.append(p)\n",
    "                if len(d_params)>0:\n",
    "                    torch.nn.utils.clip_grad_norm_(d_params,CLIP)\n",
    "                optD.step()\n",
    "\n",
    "            # --- train generator with SAM ---\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            z1=torch.randn(B,L,NOISE,device=DEVICE)*noise_scale\n",
    "            L1=_g_loss(G,D,Xb,Yb,Cb,z1,weights,pc_idx)\n",
    "            L1.backward()\n",
    "            sam.first_step()\n",
    "\n",
    "            # second forward/backward at perturbed weights\n",
    "            z2=torch.randn(B,L,NOISE,device=DEVICE)*noise_scale\n",
    "            L2=_g_loss(G,D,Xb,Yb,Cb,z2,weights,pc_idx)\n",
    "            L2.backward()\n",
    "            sam.second_step()\n",
    "\n",
    "        # --- validation / early stop ---\n",
    "        sm,cov=validate(G,Xva,Yva,Cva)\n",
    "        comp=sm+10*abs(cov-0.9)\n",
    "        if comp<best-1e-6:\n",
    "            best=comp\n",
    "            best_sd=(copy.deepcopy(G.state_dict()),copy.deepcopy(D.state_dict()))\n",
    "            wait=0\n",
    "        else:\n",
    "            wait+=1\n",
    "            if wait>=PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_sd:\n",
    "        G.load_state_dict(best_sd[0]); D.load_state_dict(best_sd[1])\n",
    "    torch.save(G.state_dict(),f\"{OUT}/generator.pt\")\n",
    "    torch.save(D.state_dict(),f\"{OUT}/critic.pt\")\n",
    "    return G,D\n",
    "\n",
    "\n",
    "# ----------------- forecast band generator -----------------\n",
    "\n",
    "def psd(matrix):\n",
    "    eigvals, eigvecs = np.linalg.eigh(matrix)\n",
    "    eigvals[eigvals < 1e-6] = 1e-6\n",
    "    return (eigvecs @ np.diag(eigvals) @ eigvecs.T).astype(np.float32)\n",
    "\n",
    "def copula_bands(G,X,C,K=K_MC):\n",
    "    \"\"\"\n",
    "    Kept for backward compatibility, but we're now relying on direct quantile heads.\n",
    "    \"\"\"\n",
    "    G.eval()\n",
    "    Xt=torch.tensor(X,dtype=torch.float32,device=DEVICE)\n",
    "    Ct=torch.tensor(C,dtype=torch.long,device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mu,ls,_=G(Xt,torch.zeros(Xt.shape[0],Xt.shape[1],NOISE,device=DEVICE),Ct)\n",
    "    mu=mu.cpu().numpy()[...,0]\n",
    "    sig=np.clip(np.exp(ls.cpu().numpy()[...,0]),1e-3,50.0)\n",
    "    sig = sig * SIG_CALIBRATION_FACTOR\n",
    "    samples=[]\n",
    "    for i in range(Xt.shape[0]):\n",
    "        L=Xt.shape[1]\n",
    "        rho=0.5\n",
    "        R=np.fromfunction(lambda a,b: rho**np.abs(a-b),(L,L))\n",
    "        cov=psd((sig[i][:,None]*sig[i][None,:])*R)\n",
    "        z=np.random.multivariate_normal(np.zeros(L),cov,size=K).astype(np.float32)\n",
    "        samples.append(mu[i][None,:]+z)\n",
    "    samples=np.stack(samples,0)\n",
    "    return np.percentile(samples,10,1),np.percentile(samples,50,1),np.percentile(samples,90,1)\n",
    "\n",
    "\n",
    "# ----------------- director tables helpers -----------------\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(self,X,Y,C): self.X=X; self.Y=Y; self.C=C\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,i): return self.X[i],self.Y[i],self.C[i]\n",
    "\n",
    "def calc_reporting_continuity(df_full_raw, seq_len=SEQ):\n",
    "    rows=[]\n",
    "    for dist, g in df_full_raw.groupby(\"District\"):\n",
    "        g = g.sort_values([\"Year\",\"Epi_Week\"])\n",
    "        recent = g.tail(seq_len)\n",
    "        reported_mask = ~recent[\"weekly_hospitalised_cases\"].isna()\n",
    "        continuity_pct = 100.0 * reported_mask.mean()\n",
    "        rows.append({\n",
    "            \"District\": dist,\n",
    "            f\"Reporting_Continuity_Last{seq_len}w(%)\": continuity_pct\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def classify_data_quality(continuity_pct):\n",
    "    if continuity_pct >= 90:\n",
    "        return \"âœ… Reliable\"\n",
    "    elif continuity_pct >= 70:\n",
    "        return \"ðŸŸ¡ Watch\"\n",
    "    else:\n",
    "        return \"ðŸ”´ Needs field check\"\n",
    "\n",
    "def classify_growth(g):\n",
    "    if g >= RAPID_GROWTH_THRESH:\n",
    "        return \"ðŸ”´ Rapid growth\"\n",
    "    elif g >= MODERATE_GROWTH_THRESH:\n",
    "        return \"ðŸŸ  Moderate growth\"\n",
    "    else:\n",
    "        return \"ðŸŸ¢ Stable/decline\"\n",
    "\n",
    "def classify_risk(high_scenario):\n",
    "    if high_scenario >= 40:\n",
    "        return \"ðŸ”´ Surge likely\"\n",
    "    elif high_scenario >= 20:\n",
    "        return \"ðŸŸ  Elevated\"\n",
    "    else:\n",
    "        return \"ðŸŸ¢ Low\"\n",
    "\n",
    "def classify_conf(width):\n",
    "    if width <= 5:\n",
    "        return \"âœ… High confidence\"\n",
    "    elif width <= 15:\n",
    "        return \"ðŸŸ¡ Medium confidence\"\n",
    "    else:\n",
    "        return \"âš  Low confidence\"\n",
    "\n",
    "def build_isochrone_table(per_district_info):\n",
    "    return pd.DataFrame(per_district_info)\n",
    "\n",
    "def calc_climate_lag_influence(\n",
    "    Xseq, Cseq, feats, next_week_pred_by_seq\n",
    "):\n",
    "    \"\"\"\n",
    "    Produce a clean, human-readable climate/lag influence table:\n",
    "      - excludes the 'prev_y' channel\n",
    "      - parses names to base_var + nice lag/roll text\n",
    "      - handles seasonality encodings\n",
    "      - sorts by |corr|\n",
    "    \"\"\"\n",
    "    # Use the \"current\" timestep features, excluding the trailing prev_y channel\n",
    "    # to mirror diarrhoeaâ€™s approach.\n",
    "    t_idx = -2\n",
    "    feature_matrix = Xseq[:, t_idx, :-1]   # drop prev_y\n",
    "    preds = np.array(next_week_pred_by_seq)\n",
    "\n",
    "    def parse_feature_name(raw_name: str):\n",
    "        \"\"\"\n",
    "        Examples:\n",
    "          Total_Rainfall_lag3    -> base_var='Total_Rainfall', lag_info='lag 3w'\n",
    "          Avg_Temperature_rmean6 -> base_var='Avg_Temperature', lag_info='6w rolling mean'\n",
    "          weekly_hospitalised_cases_rstd12 -> base_var='weekly_hospitalised_cases', lag_info='12w rolling std'\n",
    "          week_sin/cos           -> base_var='season(week)',  lag_info=''\n",
    "          month_sin/cos          -> base_var='season(month)', lag_info=''\n",
    "          Year_num               -> base_var='Year_num',      lag_info='same week'\n",
    "          Avg_NDVI               -> base_var='Avg_NDVI',      lag_info='same week'\n",
    "        \"\"\"\n",
    "        name = raw_name\n",
    "\n",
    "        # seasonality encodings\n",
    "        if name in (\"week_sin\", \"week_cos\"):\n",
    "            return \"season(week)\", \"\"\n",
    "        if name in (\"month_sin\", \"month_cos\"):\n",
    "            return \"season(month)\", \"\"\n",
    "\n",
    "        # rolling mean/std\n",
    "        if \"_rmean\" in name:\n",
    "            base, tail = name.split(\"_rmean\")\n",
    "            return base, f\"{tail}w rolling mean\"\n",
    "        if \"_rstd\" in name:\n",
    "            base, tail = name.split(\"_rstd\")\n",
    "            return base, f\"{tail}w rolling std\"\n",
    "\n",
    "        # lags\n",
    "        if \"_lag\" in name:\n",
    "            base, tail = name.split(\"_lag\")\n",
    "            return base, f\"lag {tail}w\"\n",
    "\n",
    "        # default is \"same week\"\n",
    "        return name, \"same week\"\n",
    "\n",
    "    rows = []\n",
    "    for fi, raw_feat_name in enumerate(feats):  # feats aligns with Xseq[..., :-1]\n",
    "        col_vals = feature_matrix[:, fi]\n",
    "        if np.std(col_vals) < 1e-8:\n",
    "            corr = 0.0\n",
    "        else:\n",
    "            corr = np.corrcoef(col_vals, preds)[0, 1]\n",
    "\n",
    "        base_var, lag_info = parse_feature_name(raw_feat_name)\n",
    "        rows.append({\n",
    "            \"feature\": raw_feat_name,\n",
    "            \"base_var\": base_var,\n",
    "            \"lag_info\": lag_info,\n",
    "            \"pearson_corr_with_next_week_forecast\": float(corr),\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df[\"abs_corr\"] = out_df[\"pearson_corr_with_next_week_forecast\"].abs()\n",
    "    out_df = out_df.sort_values(\"abs_corr\", ascending=False)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "# ----------------- evaluate + summarize -----------------\n",
    "\n",
    "def evaluate_and_summarize(G, Xseq, Yseq, Cseq, ysc, le, df_full_raw, feats):\n",
    "    G.eval()\n",
    "\n",
    "    X_torch = torch.tensor(Xseq, dtype=torch.float32, device=DEVICE)\n",
    "    C_torch = torch.tensor(Cseq, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu, ls, qs = G(\n",
    "            X_torch,\n",
    "            torch.zeros(X_torch.shape[0], X_torch.shape[1], NOISE, device=DEVICE),\n",
    "            C_torch\n",
    "        )\n",
    "        mp_scaled  = mu[...,0].cpu().numpy()\n",
    "        q10_scaled = qs[0][...,0].cpu().numpy()\n",
    "        q50_scaled = qs[1][...,0].cpu().numpy()\n",
    "        q90_scaled = qs[2][...,0].cpu().numpy()\n",
    "\n",
    "    def inv_scale(seq_arr_scaled, scaler):\n",
    "        seq_arr_scaled = seq_arr_scaled.reshape(-1,1)\n",
    "        unstd = scaler.inverse_transform(seq_arr_scaled).reshape(-1)\n",
    "        return np.clip(np.expm1(unstd), 0, None)\n",
    "\n",
    "    district_names = []\n",
    "    last_actual_list = []\n",
    "    current_actual_list = []\n",
    "    current_pred_list = []\n",
    "    next_week_pred_list = []\n",
    "    next_week_hi_list = []\n",
    "    next_week_lo_list = []\n",
    "    growth_rate_list = []\n",
    "    conf_width_list = []\n",
    "    peak_week_list = []\n",
    "    peak_value_list = []\n",
    "    peak_hi_list = []\n",
    "    peak_leadtime_list = []\n",
    "    epiweek_meta_rows = []\n",
    "\n",
    "    yF_all=[]; mF_all=[]; lF_all=[]; hF_all=[]\n",
    "\n",
    "    idx_curr = -2\n",
    "    idx_next = -1\n",
    "\n",
    "    last_epi_meta = {}\n",
    "\n",
    "    for i, cid in enumerate(Cseq):\n",
    "        scaler = ysc[int(cid)]\n",
    "        dist_name = le.inverse_transform([cid])[0]\n",
    "\n",
    "        y_real  = inv_scale(Yseq[i].reshape(-1),     scaler)\n",
    "        m_real  = inv_scale(mp_scaled[i].reshape(-1),scaler)\n",
    "        lo_real = inv_scale(q10_scaled[i].reshape(-1),scaler)\n",
    "        hi_real = inv_scale(q90_scaled[i].reshape(-1),scaler)\n",
    "\n",
    "        yF_all.append(y_real)\n",
    "        mF_all.append(m_real)\n",
    "        lF_all.append(lo_real)\n",
    "        hF_all.append(hi_real)\n",
    "\n",
    "        safe_idx_curr = idx_curr if idx_curr >= -len(y_real) else -1\n",
    "        safe_idx_next = idx_next if idx_next >= -len(y_real) else -1\n",
    "\n",
    "        actual_curr = y_real[safe_idx_curr]\n",
    "        pred_curr   = m_real[safe_idx_curr]\n",
    "        actual_prev = y_real[safe_idx_curr-1] if (safe_idx_curr-1)>=-len(y_real) else y_real[safe_idx_curr]\n",
    "\n",
    "        pred_next   = m_real[safe_idx_next]\n",
    "        hi_next     = hi_real[safe_idx_next]\n",
    "        lo_next     = lo_real[safe_idx_next]\n",
    "\n",
    "        growth = (pred_next - actual_prev) / (actual_prev + 1e-6)\n",
    "        conf_width = hi_next - lo_next\n",
    "\n",
    "        look_slice = m_real[safe_idx_next-PEAK_LOOKAHEAD_WEEKS+1 : safe_idx_next+1]\n",
    "        hi_slice   = hi_real[safe_idx_next-PEAK_LOOKAHEAD_WEEKS+1 : safe_idx_next+1]\n",
    "        if len(look_slice)==0:\n",
    "            peak_when=\"NA\"; peak_val=np.nan; peak_hi=np.nan; lead_weeks=np.nan\n",
    "        else:\n",
    "            local_max_idx = int(np.argmax(look_slice))\n",
    "            peak_val = look_slice[local_max_idx]\n",
    "            peak_hi  = hi_slice[local_max_idx]\n",
    "            lead_weeks = (len(look_slice)-1) - local_max_idx\n",
    "            peak_when = f\"t+{lead_weeks}w\"\n",
    "\n",
    "        gdist = df_full_raw[df_full_raw[\"District\"]==dist_name].sort_values([\"Year\",\"Epi_Week\"])\n",
    "        if len(gdist) > 0:\n",
    "            lastrow = gdist.iloc[-1]\n",
    "            last_year = int(lastrow[\"Year\"])\n",
    "            last_epi  = int(lastrow[\"Epi_Week\"])\n",
    "        else:\n",
    "            last_year = None\n",
    "            last_epi  = None\n",
    "\n",
    "        district_names.append(dist_name)\n",
    "        last_actual_list.append(actual_prev)\n",
    "        current_actual_list.append(actual_curr)\n",
    "        current_pred_list.append(pred_curr)\n",
    "        next_week_pred_list.append(pred_next)\n",
    "        next_week_hi_list.append(hi_next)\n",
    "        next_week_lo_list.append(lo_next)\n",
    "        growth_rate_list.append(growth)\n",
    "        conf_width_list.append(conf_width)\n",
    "        peak_week_list.append(peak_when)\n",
    "        peak_value_list.append(peak_val)\n",
    "        peak_hi_list.append(peak_hi)\n",
    "        peak_leadtime_list.append(lead_weeks)\n",
    "\n",
    "        last_epi_meta[dist_name] = {\n",
    "            \"Year\": last_year,\n",
    "            \"Epi_Week\": last_epi\n",
    "        }\n",
    "\n",
    "        epiweek_meta_rows.append({\n",
    "            \"District\": dist_name,\n",
    "            \"Year\": last_year,\n",
    "            \"Epi_Week\": last_epi,\n",
    "            \"Forecast_next_week_median\": float(pred_next),\n",
    "            \"Forecast_next_week_hi\": float(hi_next),\n",
    "            \"Growth_flag\": classify_growth(growth)\n",
    "        })\n",
    "\n",
    "    yF_all = np.concatenate(yF_all)\n",
    "    mF_all = np.concatenate(mF_all)\n",
    "    lF_all = np.concatenate(lF_all)\n",
    "    hF_all = np.concatenate(hF_all)\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"SMAPE\": smape(yF_all, mF_all),\n",
    "        \"MSE\": mean_squared_error(yF_all, mF_all),\n",
    "        \"RMSE\": math.sqrt(mean_squared_error(yF_all, mF_all)),\n",
    "        \"R2\": r2_score(yF_all, mF_all),\n",
    "        \"Coverage90\": float(np.mean((yF_all >= lF_all) & (yF_all <= hF_all))),\n",
    "    }\n",
    "\n",
    "    quality_df_raw = calc_reporting_continuity(df_full_raw, seq_len=SEQ)\n",
    "    quality_df = pd.merge(\n",
    "        pd.DataFrame({\"District\": district_names}),\n",
    "        quality_df_raw,\n",
    "        on=\"District\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    cont_col = f\"Reporting_Continuity_Last{SEQ}w(%)\"\n",
    "    quality_df[\"Data_quality_flag\"] = quality_df[cont_col].apply(classify_data_quality)\n",
    "\n",
    "    watchlist_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Expected_cases_next_week\": np.round(next_week_pred_list,2),\n",
    "        \"High_scenario_p90\": np.round(next_week_hi_list,2),\n",
    "    })\n",
    "    watchlist_df[\"Status\"] = watchlist_df[\"High_scenario_p90\"].apply(classify_risk)\n",
    "    watchlist_df = watchlist_df.sort_values(\"High_scenario_p90\", ascending=False)\n",
    "\n",
    "    capacity_lookup = {d: DEFAULT_CAPACITY_PER_DISTRICT for d in district_names}\n",
    "    overflow_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Capacity_threshold_beds_per_week\": [capacity_lookup[d] for d in district_names],\n",
    "        \"Forecast_median_next_week\": np.round(next_week_pred_list,2),\n",
    "        \"High_scenario_p90\": np.round(next_week_hi_list,2),\n",
    "    })\n",
    "    overflow_df[\"Breach_risk_flag\"] = [\n",
    "        \"YES\" if hi > capacity_lookup[d] else \"NO\"\n",
    "        for d, hi in zip(district_names, next_week_hi_list)\n",
    "    ]\n",
    "    overflow_df = overflow_df.sort_values(\"High_scenario_p90\", ascending=False)\n",
    "\n",
    "    accel_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Last_week_cases\": np.round(last_actual_list,2),\n",
    "        \"This_week_actual\": np.round(current_actual_list,2),\n",
    "        \"This_week_predicted\": np.round(current_pred_list,2),\n",
    "        \"Next_week_forecast\": np.round(next_week_pred_list,2),\n",
    "        \"Growth_rate_WoW(%)\": np.round(np.array(growth_rate_list)*100,1),\n",
    "    })\n",
    "    accel_df[\"Growth_flag\"] = accel_df[\"Growth_rate_WoW(%)\"].apply(lambda pct: classify_growth(pct/100.0))\n",
    "    accel_df = accel_df.sort_values(\"Growth_rate_WoW(%)\", ascending=False)\n",
    "\n",
    "    confidence_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Forecast_next_week\": np.round(next_week_pred_list,2),\n",
    "        \"Uncertainty_width(p90-p10)\": np.round(conf_width_list,2),\n",
    "    })\n",
    "    confidence_df[\"Confidence_flag\"] = confidence_df[\"Uncertainty_width(p90-p10)\"].apply(classify_conf)\n",
    "    confidence_df = confidence_df.sort_values(\"Uncertainty_width(p90-p10)\", ascending=True)\n",
    "\n",
    "    peak_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Peak_lead_time_weeks\": peak_leadtime_list,\n",
    "        \"Peak_cases_median\": np.round(peak_value_list,2),\n",
    "        \"Peak_cases_high(p90)\": np.round(peak_hi_list,2),\n",
    "        \"Peak_when\": peak_week_list\n",
    "    }).sort_values(\"Peak_cases_median\", ascending=False)\n",
    "\n",
    "    isochrone_df = build_isochrone_table(epiweek_meta_rows)\n",
    "\n",
    "    gap_pct_list = []\n",
    "    for a, p in zip(current_actual_list, current_pred_list):\n",
    "        gap_pct = (p - a)/(a + 1e-6)*100.0\n",
    "        gap_pct_list.append(gap_pct)\n",
    "    nowcast_gap_df = pd.DataFrame({\n",
    "        \"District\": district_names,\n",
    "        \"Current_week_actual\": np.round(current_actual_list,2),\n",
    "        \"Current_week_predicted_from_prev\": np.round(current_pred_list,2),\n",
    "        \"Nowcast_gap_percent\": np.round(gap_pct_list,1),\n",
    "    }).sort_values(\"Nowcast_gap_percent\", ascending=False)\n",
    "\n",
    "    climate_influence_df = calc_climate_lag_influence(\n",
    "    Xseq, Cseq, feats, next_week_pred_list\n",
    ")\n",
    "\n",
    "    top5 = watchlist_df.head(5)[[\"District\",\"Expected_cases_next_week\",\"High_scenario_p90\",\"Status\"]].to_dict(orient=\"records\")\n",
    "    rapid = accel_df[accel_df[\"Growth_flag\"]==\"ðŸ”´ Rapid growth\"][\"District\"].tolist()\n",
    "    overflow_risk = overflow_df[overflow_df[\"Breach_risk_flag\"]==\"YES\"][\"District\"].tolist()\n",
    "    dq_bad = quality_df[quality_df[\"Data_quality_flag\"]==\"ðŸ”´ Needs field check\"][\"District\"].tolist()\n",
    "    summary_text = {\n",
    "        \"Top5_high_risk_next_week\": to_python(top5),\n",
    "        \"Districts_with_rapid_growth\": to_python(rapid),\n",
    "        \"Districts_with_capacity_breach_risk\": to_python(overflow_risk),\n",
    "        \"Districts_with_data_quality_issues\": to_python(dq_bad),\n",
    "        \"Model_calibration\": {\n",
    "            \"Coverage90\": round(float(overall_metrics[\"Coverage90\"]),3),\n",
    "            \"SMAPE\": round(float(overall_metrics[\"SMAPE\"]),2),\n",
    "            \"R2\": round(float(overall_metrics[\"R2\"]),3),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    products = {\n",
    "        \"watchlist_df\": watchlist_df,\n",
    "        \"overflow_df\": overflow_df,\n",
    "        \"accel_df\": accel_df,\n",
    "        \"confidence_df\": confidence_df,\n",
    "        \"quality_df\": quality_df,\n",
    "        \"peak_df\": peak_df,\n",
    "        \"isochrone_df\": isochrone_df,\n",
    "        \"nowcast_gap_df\": nowcast_gap_df,\n",
    "        \"climate_influence_df\": climate_influence_df,\n",
    "        \"executive_summary\": summary_text,\n",
    "        \"overall_metrics\": overall_metrics,\n",
    "        \"last_epi_meta\": last_epi_meta,\n",
    "    }\n",
    "    return overall_metrics, products\n",
    "\n",
    "\n",
    "# ----------------- push results to DB (UNCHANGED OUTPUT SCHEMA) -----------------\n",
    "\n",
    "def push_products_to_db(conn, products):\n",
    "    last_epi_meta = products[\"last_epi_meta\"]\n",
    "\n",
    "    # WATCHLIST\n",
    "    watchlist_rows = []\n",
    "    for _, r in products[\"watchlist_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        watchlist_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Expected_cases_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Status\"]\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_watchlist\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"expected_cases_next_week\",\"high_scenario_p90\",\"status\"],\n",
    "        watchlist_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # OVERFLOW RISK\n",
    "    overflow_rows = []\n",
    "    for _, r in products[\"overflow_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        overflow_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            int(r[\"Capacity_threshold_beds_per_week\"]) if not pd.isna(r[\"Capacity_threshold_beds_per_week\"]) else None,\n",
    "            py(r[\"Forecast_median_next_week\"]),\n",
    "            py(r[\"High_scenario_p90\"]),\n",
    "            r[\"Breach_risk_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_overflow_risk\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"capacity_threshold_beds_per_week\",\"forecast_median_next_week\",\"high_scenario_p90\",\"breach_risk_flag\"],\n",
    "        overflow_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # ACCELERATION ALERTS\n",
    "    accel_rows = []\n",
    "    for _, r in products[\"accel_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        accel_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Last_week_cases\"]),\n",
    "            py(r[\"This_week_actual\"]),\n",
    "            py(r[\"This_week_predicted\"]),\n",
    "            py(r[\"Next_week_forecast\"]),\n",
    "            py(r[\"Growth_rate_WoW(%)\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_acceleration_alerts\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"last_week_cases\",\"this_week_actual\",\"this_week_predicted\",\"next_week_forecast\",\"growth_rate_wow\",\"growth_flag\"],\n",
    "        accel_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # CONFIDENCE\n",
    "    conf_rows = []\n",
    "    for _, r in products[\"confidence_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        conf_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Forecast_next_week\"]),\n",
    "            py(r[\"Uncertainty_width(p90-p10)\"]),\n",
    "            r[\"Confidence_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_confidence\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"forecast_next_week\",\"uncertainty_width\",\"confidence_flag\"],\n",
    "        conf_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # QUALITY\n",
    "    qual_rows = []\n",
    "    rep_col = [c for c in products[\"quality_df\"].columns if \"Reporting_Continuity_Last\" in c][0]\n",
    "    for _, r in products[\"quality_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        qual_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[rep_col]),\n",
    "            r[\"Data_quality_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_surveillance_quality\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"reporting_continuity_pct\",\"data_quality_flag\"],\n",
    "        qual_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # PEAK\n",
    "    peak_rows = []\n",
    "    for _, r in products[\"peak_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        peak_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Peak_lead_time_weeks\"]),\n",
    "            py(r[\"Peak_cases_median\"]),\n",
    "            py(r[\"Peak_cases_high(p90)\"]),\n",
    "            r[\"Peak_when\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_peak_projection\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"peak_lead_time_weeks\",\"peak_cases_median\",\"peak_cases_high_p90\",\"peak_when\"],\n",
    "        peak_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # ISOCHRONE\n",
    "    iso_rows = []\n",
    "    for _, r in products[\"isochrone_df\"].iterrows():\n",
    "        iso_rows.append((\n",
    "            r[\"District\"],\n",
    "            r[\"Year\"],\n",
    "            r[\"Epi_Week\"],\n",
    "            py(r[\"Forecast_next_week_median\"]),\n",
    "            py(r[\"Forecast_next_week_hi\"]),\n",
    "            r[\"Growth_flag\"],\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_isochrone_spread\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"forecast_next_week_median\",\"forecast_next_week_hi\",\"growth_flag\"],\n",
    "        iso_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # NOWCAST GAP\n",
    "    gap_rows = []\n",
    "    for _, r in products[\"nowcast_gap_df\"].iterrows():\n",
    "        dm = last_epi_meta.get(r[\"District\"], {})\n",
    "        gap_rows.append((\n",
    "            r[\"District\"],\n",
    "            dm.get(\"Year\"),\n",
    "            dm.get(\"Epi_Week\"),\n",
    "            py(r[\"Current_week_actual\"]),\n",
    "            py(r[\"Current_week_predicted_from_prev\"]),\n",
    "            py(r[\"Nowcast_gap_percent\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_nowcast_gap\",\n",
    "        [\"district\",\"year\",\"epi_week\",\"current_week_actual\",\"current_week_predicted_from_prev\",\"nowcast_gap_percent\"],\n",
    "        gap_rows,\n",
    "        pk_cols=[\"district\",\"year\",\"epi_week\"],\n",
    "        wipe_first=False\n",
    "    )\n",
    "\n",
    "    # CLIMATE INFLUENCE\n",
    "    clim_rows = []\n",
    "    for _, r in products[\"climate_influence_df\"].iterrows():\n",
    "        clim_rows.append((\n",
    "            r[\"feature\"],\n",
    "            r[\"base_var\"],\n",
    "            r[\"lag_info\"],\n",
    "            py(r[\"pearson_corr_with_next_week_forecast\"]),\n",
    "            py(r[\"abs_corr\"]),\n",
    "        ))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_climate_influence\",\n",
    "        [\"feature\",\"base_var\",\"lag_info\",\"pearson_corr_with_next_week_forecast\",\"abs_corr\"],\n",
    "        clim_rows,\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "    # EXEC SUMMARY\n",
    "    summary_json = json.dumps(to_python(products[\"executive_summary\"]))\n",
    "    upsert_table(\n",
    "        conn,\n",
    "        \"dengue_exec_summary\",\n",
    "        [\"summary\"],\n",
    "        [(summary_json,)],\n",
    "        pk_cols=None,\n",
    "        wipe_first=True\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------- main -----------------\n",
    "\n",
    "def main():\n",
    "    print(\"1) Load weekly data from Postgres\")\n",
    "    df = load_weekly_from_db()\n",
    "\n",
    "    # engineered features BEFORE lag/roll\n",
    "    df = add_time_features(df)\n",
    "    df = add_incidence_derivs(df)           # surge velocity features\n",
    "    df = add_static_district_feats(df)      # district-level capacity proxy\n",
    "\n",
    "    base_series = [\n",
    "        \"weekly_hospitalised_cases\",\n",
    "        \"Total_Rainfall\",\n",
    "        \"Avg_Humidity\",\n",
    "        \"Avg_Temperature\",\n",
    "        \"case_growth_1w\",\n",
    "        \"case_growth_2w\",\n",
    "        \"acceleration\",\n",
    "        \"district_capacity_proxy\"\n",
    "    ]\n",
    "    if \"Avg_NDVI\" in df.columns:\n",
    "        base_series.append(\"Avg_NDVI\")\n",
    "    if \"Avg_NDWI\" in df.columns:\n",
    "        base_series.append(\"Avg_NDWI\")\n",
    "\n",
    "    df = add_lags_rolls(\n",
    "        df,\n",
    "        base_cols=base_series,\n",
    "        lags=(1, 3, 6, 12),\n",
    "        rolls=(3, 6, 12),\n",
    "    )\n",
    "\n",
    "    feats = [\n",
    "        \"Total_Rainfall\",\n",
    "        \"Avg_Humidity\",\n",
    "        \"Avg_Temperature\",\n",
    "        \"case_growth_1w\",\n",
    "        \"case_growth_2w\",\n",
    "        \"acceleration\",\n",
    "        \"district_capacity_proxy\",\n",
    "        \"Year_num\",\n",
    "        \"week_sin\",\n",
    "        \"week_cos\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "    ]\n",
    "    for optional in [\"Avg_NDVI\", \"Avg_NDWI\"]:\n",
    "        if optional in df.columns:\n",
    "            feats.append(optional)\n",
    "\n",
    "    feats += [c for c in df.columns if any(tag in c for tag in [\"_lag\", \"_rmean\", \"_rstd\"])]\n",
    "\n",
    "    target = \"weekly_hospitalised_cases\"\n",
    "\n",
    "    print(\"2) Prepare train/val/test splits\")\n",
    "    tr_parts, va_parts, te_parts = [], [], []\n",
    "    for name, group in df.groupby(\"District\"):\n",
    "        group = group.sort_values([\"Year\", \"Epi_Week\"])\n",
    "        if len(group) > (VAL_H_WEEKS + TEST_H_WEEKS):\n",
    "            te_parts.append(group.iloc[-TEST_H_WEEKS:])\n",
    "            va_parts.append(group.iloc[-(VAL_H_WEEKS + TEST_H_WEEKS) : -TEST_H_WEEKS])\n",
    "            tr_parts.append(group.iloc[: -(VAL_H_WEEKS + TEST_H_WEEKS)])\n",
    "        elif len(group) > TEST_H_WEEKS:\n",
    "            te_parts.append(group.iloc[-TEST_H_WEEKS:])\n",
    "            tr_parts.append(group.iloc[:-TEST_H_WEEKS])\n",
    "        else:\n",
    "            tr_parts.append(group)\n",
    "\n",
    "    tr_df = pd.concat(tr_parts).reset_index(drop=True)\n",
    "    va_df = pd.concat(va_parts).reset_index(drop=True) if va_parts else tr_df.iloc[0:0].copy()\n",
    "    te_df = pd.concat(te_parts).reset_index(drop=True) if te_parts else tr_df.iloc[0:0].copy()\n",
    "\n",
    "    df_full_raw = df.copy()\n",
    "\n",
    "    print(\"3) Fit encoders/scalers on TRAIN\")\n",
    "    split_tr, le_tr, fsc_tr, ysc_tr = build_mats(tr_df, feats, target)\n",
    "\n",
    "    X_tr, Y_tr, C_tr = to_seq(split_tr, SEQ, prev_y=True)\n",
    "    if X_tr.shape[0] == 0:\n",
    "        raise RuntimeError(\"No train sequences. Reduce SEQ or inspect data continuity.\")\n",
    "    print(f\"Train sequences: {X_tr.shape}\")\n",
    "\n",
    "    if len(va_df) > 0:\n",
    "        split_va = apply_mats(va_df, feats, target, le_tr, fsc_tr, ysc_tr)\n",
    "        X_va, Y_va, C_va = to_seq(split_va, SEQ, prev_y=True)\n",
    "        if X_va.shape[0] == 0:\n",
    "            X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "    else:\n",
    "        X_va, Y_va, C_va = X_tr, Y_tr, C_tr\n",
    "\n",
    "    if len(te_df) > 0:\n",
    "        split_te = apply_mats(te_df, feats, target, le_tr, fsc_tr, ysc_tr)\n",
    "        X_te, Y_te, C_te = to_seq(split_te, SEQ, prev_y=True)\n",
    "        if X_te.shape[0] == 0:\n",
    "            X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "    else:\n",
    "        X_te, Y_te, C_te = X_va, Y_va, C_va\n",
    "\n",
    "    nc = int(C_tr.max()) + 1\n",
    "    cond_dim = X_tr.shape[2]\n",
    "\n",
    "    pc_idx = [\n",
    "        i for i, feature in enumerate(feats + [\"prev_y\"])\n",
    "        if feature in (\"week_sin\", \"week_cos\", \"month_sin\", \"month_cos\")\n",
    "    ]\n",
    "\n",
    "    print(\"4) Train GAN (train) + Early stop (val)\")\n",
    "    G, D = train_gan(X_tr, Y_tr, C_tr, X_va, Y_va, C_va, cond_dim, nc, pc_idx)\n",
    "\n",
    "    print(\"5) Build director tables on HELD-OUT TEST\")\n",
    "    overall_metrics, products = evaluate_and_summarize(\n",
    "        G, X_te, Y_te, C_te, ysc_tr, le_tr, df_full_raw, feats\n",
    "    )\n",
    "\n",
    "    # save locally\n",
    "    products[\"watchlist_df\"].to_csv(f\"{OUT}/watchlist.csv\", index=False)\n",
    "    products[\"overflow_df\"].to_csv(f\"{OUT}/overflow_risk.csv\", index=False)\n",
    "    products[\"accel_df\"].to_csv(f\"{OUT}/acceleration_alerts.csv\", index=False)\n",
    "    products[\"confidence_df\"].to_csv(f\"{OUT}/forecast_confidence.csv\", index=False)\n",
    "    products[\"quality_df\"].to_csv(f\"{OUT}/surveillance_quality.csv\", index=False)\n",
    "    products[\"peak_df\"].to_csv(f\"{OUT}/peak_projection.csv\", index=False)\n",
    "    products[\"isochrone_df\"].to_csv(f\"{OUT}/isochrone_spread.csv\", index=False)\n",
    "    products[\"nowcast_gap_df\"].to_csv(f\"{OUT}/nowcast_gap.csv\", index=False)\n",
    "    products[\"climate_influence_df\"].to_csv(f\"{OUT}/climate_lag_influence.csv\", index=False)\n",
    "\n",
    "    dash_manifest = {\n",
    "        \"executive_summary\": products[\"executive_summary\"],\n",
    "        \"overall_metrics\": products[\"overall_metrics\"],\n",
    "        \"notes\": \"Auto-generated decision tables for dengue surge planning, including climate lag screen, spatial isochrone prep, and nowcast gap.\"\n",
    "    }\n",
    "    dash_manifest_clean = to_python(dash_manifest)\n",
    "    with open(f\"{OUT}/DASHBOARD_SUMMARY.json\",\"w\") as f:\n",
    "        json.dump(dash_manifest_clean, f, indent=2)\n",
    "\n",
    "    print(\"6) Create / upsert output tables in Postgres\")\n",
    "    conn = get_db_conn()\n",
    "    ensure_output_tables(conn)\n",
    "    push_products_to_db(conn, products)\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\n=== Executive Summary Preview ===\")\n",
    "    print(json.dumps(to_python(dash_manifest[\"executive_summary\"]), indent=2))\n",
    "    print(\"\\nArtifacts saved to\", OUT, \"and pushed to Postgres.\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
